{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "673ff5b2-3d7b-488e-a3a7-189ef9230d6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96163 images belonging to 10 classes.\n",
      "Found 16132 images belonging to 10 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_11\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_11\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100352</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">51,380,736</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_11 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_44 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_44 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_45 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_45 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_46 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_46 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_47 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_47 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_11 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100352\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m51,380,736\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">52,931,712</span> (201.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m52,931,712\u001b[0m (201.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">52,931,712</span> (201.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m52,931,712\u001b[0m (201.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m3005/3005\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3163s\u001b[0m 1s/step - loss: 1.5594e-06 - val_loss: 0.0000e+00\n",
      "Epoch 2/50\n",
      "\u001b[1m3005/3005\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3153s\u001b[0m 1s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 3/50\n",
      "\u001b[1m3005/3005\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3144s\u001b[0m 1s/step - loss: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 4/50\n",
      "\u001b[1m1899/3005\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m18:26\u001b[0m 1s/step - loss: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m batch_x, batch_y  \u001b[38;5;66;03m# Yield the images and dummy labels\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Train the encoder using the custom generator\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m history_encoder \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     66\u001b[0m     custom_data_generator(train_generator),\n\u001b[1;32m     67\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39mtrain_generator\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m train_generator\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m     68\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     69\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mcustom_data_generator(validation_generator),\n\u001b[1;32m     70\u001b[0m     validation_steps\u001b[38;5;241m=\u001b[39mvalidation_generator\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m validation_generator\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m     71\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Print losses for the encoder\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoder Training Losses:\u001b[39m\u001b[38;5;124m\"\u001b[39m, history_encoder\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[1;32m   1553\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1554\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   1555\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1556\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[1;32m   1557\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1558\u001b[0m   )\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1567\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "# Set data directories\n",
    "train_data_dir = 'Dataset/augmented_training'\n",
    "val_data_dir = 'Dataset/validation'\n",
    "\n",
    "# ImageDataGenerator for training and validation sets\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create the data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=None,  # No labels for the encoder\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    val_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=None,  # No labels for the encoder\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Define the encoder model\n",
    "input_img = layers.Input(shape=(224, 224, 3))\n",
    "\n",
    "# Encoder (Feature extraction)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# Latent Space\n",
    "latent_space = layers.Flatten()(x)\n",
    "latent_space = layers.Dense(512, activation='relu')(latent_space)  # Latent space size of 512\n",
    "\n",
    "# Create the encoder model\n",
    "encoder = models.Model(input_img, latent_space)\n",
    "\n",
    "# Compile the encoder model\n",
    "encoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Show the model architecture\n",
    "encoder.summary()\n",
    "\n",
    "# Custom data generator for training\n",
    "def custom_data_generator(generator):\n",
    "    while True:\n",
    "        batch_x = next(generator)  # Get the next batch of images\n",
    "        batch_y = np.zeros((batch_x.shape[0], 512))  # Dummy labels with shape (batch_size, 512)\n",
    "        yield batch_x, batch_y  # Yield the images and dummy labels\n",
    "\n",
    "# Train the encoder using the custom generator\n",
    "history_encoder = encoder.fit(\n",
    "    custom_data_generator(train_generator),\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    epochs=50,\n",
    "    validation_data=custom_data_generator(validation_generator),\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Print losses for the encoder\n",
    "print(\"Encoder Training Losses:\", history_encoder.history['loss'])\n",
    "print(\"Encoder Validation Losses:\", history_encoder.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9f2e678-89f5-42d4-9d38-f262c6a755d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.002775\n",
      "Validation Loss: 0.001146\n",
      "Epoch [2/50], Loss: 0.000725\n",
      "Validation Loss: 0.000860\n",
      "Epoch [3/50], Loss: 0.000644\n",
      "Validation Loss: 0.000420\n",
      "Epoch [4/50], Loss: 0.000634\n",
      "Validation Loss: 0.001048\n",
      "Epoch [5/50], Loss: 0.000633\n",
      "Validation Loss: 0.000644\n",
      "Epoch [6/50], Loss: 0.000631\n",
      "Validation Loss: 0.001204\n",
      "Epoch [7/50], Loss: 0.000630\n",
      "Validation Loss: 0.000562\n",
      "Epoch [8/50], Loss: 0.000631\n",
      "Validation Loss: 0.001536\n",
      "Epoch [9/50], Loss: 0.000632\n",
      "Validation Loss: 0.000313\n",
      "Epoch [10/50], Loss: 0.000630\n",
      "Validation Loss: 0.001348\n",
      "Epoch [11/50], Loss: 0.000631\n",
      "Validation Loss: 0.000690\n",
      "Epoch [12/50], Loss: 0.000635\n",
      "Validation Loss: 0.001239\n",
      "Epoch [13/50], Loss: 0.000629\n",
      "Validation Loss: 0.000424\n",
      "Epoch [14/50], Loss: 0.000629\n",
      "Validation Loss: 0.000545\n",
      "Epoch [15/50], Loss: 0.000632\n",
      "Validation Loss: 0.001293\n",
      "Epoch [16/50], Loss: 0.000633\n",
      "Validation Loss: 0.000869\n",
      "Epoch [17/50], Loss: 0.000631\n",
      "Validation Loss: 0.000486\n",
      "Epoch [18/50], Loss: 0.000632\n",
      "Validation Loss: 0.000457\n",
      "Epoch [19/50], Loss: 0.000632\n",
      "Validation Loss: 0.000510\n",
      "Epoch [20/50], Loss: 0.000630\n",
      "Validation Loss: 0.000480\n",
      "Epoch [21/50], Loss: 0.000632\n",
      "Validation Loss: 0.000380\n",
      "Epoch [22/50], Loss: 0.000632\n",
      "Validation Loss: 0.000569\n",
      "Epoch [23/50], Loss: 0.000629\n",
      "Validation Loss: 0.000405\n",
      "Epoch [24/50], Loss: 0.000632\n",
      "Validation Loss: 0.000887\n",
      "Epoch [25/50], Loss: 0.000631\n",
      "Validation Loss: 0.000343\n",
      "Epoch [26/50], Loss: 0.000632\n",
      "Validation Loss: 0.001013\n",
      "Epoch [27/50], Loss: 0.000632\n",
      "Validation Loss: 0.001829\n",
      "Epoch [28/50], Loss: 0.000632\n",
      "Validation Loss: 0.000356\n",
      "Epoch [29/50], Loss: 0.000629\n",
      "Validation Loss: 0.000460\n",
      "Epoch [30/50], Loss: 0.000635\n",
      "Validation Loss: 0.000535\n",
      "Epoch [31/50], Loss: 0.000628\n",
      "Validation Loss: 0.000524\n",
      "Epoch [32/50], Loss: 0.000631\n",
      "Validation Loss: 0.001087\n",
      "Epoch [33/50], Loss: 0.000632\n",
      "Validation Loss: 0.000252\n",
      "Epoch [34/50], Loss: 0.000633\n",
      "Validation Loss: 0.001256\n",
      "Epoch [35/50], Loss: 0.000630\n",
      "Validation Loss: 0.000492\n",
      "Epoch [36/50], Loss: 0.000634\n",
      "Validation Loss: 0.000478\n",
      "Epoch [37/50], Loss: 0.000629\n",
      "Validation Loss: 0.000680\n",
      "Epoch [38/50], Loss: 0.000632\n",
      "Validation Loss: 0.000648\n",
      "Epoch [39/50], Loss: 0.000631\n",
      "Validation Loss: 0.000938\n",
      "Epoch [40/50], Loss: 0.000630\n",
      "Validation Loss: 0.001008\n",
      "Epoch [41/50], Loss: 0.000633\n",
      "Validation Loss: 0.000561\n",
      "Epoch [42/50], Loss: 0.000631\n",
      "Validation Loss: 0.000570\n",
      "Epoch [43/50], Loss: 0.000630\n",
      "Validation Loss: 0.000667\n",
      "Epoch [44/50], Loss: 0.000631\n",
      "Validation Loss: 0.000841\n",
      "Epoch [45/50], Loss: 0.000631\n",
      "Validation Loss: 0.000991\n",
      "Epoch [46/50], Loss: 0.000633\n",
      "Validation Loss: 0.000713\n",
      "Epoch [47/50], Loss: 0.000633\n",
      "Validation Loss: 0.000436\n",
      "Epoch [48/50], Loss: 0.000631\n",
      "Validation Loss: 0.000657\n",
      "Epoch [49/50], Loss: 0.000630\n",
      "Validation Loss: 0.000590\n",
      "Epoch [50/50], Loss: 0.000631\n",
      "Validation Loss: 0.000555\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "latent_space_size = 1024  # You can adjust this if needed\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Dataset paths\n",
    "train_dir = 'Dataset/augmented_training'  # Replace with your training directory path\n",
    "val_dir = 'Dataset/validation'      # Replace with your validation directory path\n",
    "\n",
    "# Datasets\n",
    "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ResNet Encoder\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        # Remove the fully connected layer\n",
    "        self.resnet = nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "        # Use the number of features from resnet.fc\n",
    "        num_features = models.resnet50().fc.in_features\n",
    "        self.fc = nn.Linear(num_features, latent_space_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.fc(x)              # Map to latent space\n",
    "        return x\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "encoder = ResNetEncoder().to(device)\n",
    "criterion = nn.MSELoss()  # Dummy loss since we're not using true labels\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        latent_space = encoder(images)  # Get latent space\n",
    "        loss = criterion(latent_space, torch.zeros_like(latent_space))  # Dummy target\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}')\n",
    "\n",
    "    # Validation loop\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for images, _ in val_loader:\n",
    "            images = images.to(device)\n",
    "            latent_space = encoder(images)  # Get latent space\n",
    "            loss = criterion(latent_space, torch.zeros_like(latent_space))  # Dummy target\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f'Validation Loss: {avg_val_loss:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecdf633a-cfd5-4db0-afe9-4e3a7346c664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 0.0023\n",
      "Epoch [1/50], Validation Loss: 0.0027\n",
      "Epoch [2/50], Training Loss: 0.0000\n",
      "Epoch [2/50], Validation Loss: 0.0000\n",
      "Epoch [3/50], Training Loss: 0.0000\n",
      "Epoch [3/50], Validation Loss: 0.0000\n",
      "Epoch [4/50], Training Loss: 0.0000\n",
      "Epoch [4/50], Validation Loss: 0.0000\n",
      "Epoch [5/50], Training Loss: 0.0000\n",
      "Epoch [5/50], Validation Loss: 0.0000\n",
      "Epoch [6/50], Training Loss: 0.0000\n",
      "Epoch [6/50], Validation Loss: 0.0000\n",
      "Epoch [7/50], Training Loss: 0.0000\n",
      "Epoch [7/50], Validation Loss: 0.0000\n",
      "Epoch [8/50], Training Loss: 0.0000\n",
      "Epoch [8/50], Validation Loss: 0.0000\n",
      "Epoch [9/50], Training Loss: 0.0000\n",
      "Epoch [9/50], Validation Loss: 0.0000\n",
      "Epoch [10/50], Training Loss: 0.0000\n",
      "Epoch [10/50], Validation Loss: 0.0000\n",
      "Epoch [11/50], Training Loss: 0.0000\n",
      "Epoch [11/50], Validation Loss: 0.0000\n",
      "Epoch [12/50], Training Loss: 0.0000\n",
      "Epoch [12/50], Validation Loss: 0.0000\n",
      "Epoch [13/50], Training Loss: 0.0000\n",
      "Epoch [13/50], Validation Loss: 0.0000\n",
      "Epoch [14/50], Training Loss: 0.0000\n",
      "Epoch [14/50], Validation Loss: 0.0000\n",
      "Epoch [15/50], Training Loss: 0.0000\n",
      "Epoch [15/50], Validation Loss: 0.0000\n",
      "Epoch [16/50], Training Loss: 0.0000\n",
      "Epoch [16/50], Validation Loss: 0.0000\n",
      "Epoch [17/50], Training Loss: 0.0000\n",
      "Epoch [17/50], Validation Loss: 0.0000\n",
      "Epoch [18/50], Training Loss: 0.0000\n",
      "Epoch [18/50], Validation Loss: 0.0000\n",
      "Epoch [19/50], Training Loss: 0.0000\n",
      "Epoch [19/50], Validation Loss: 0.0000\n",
      "Epoch [20/50], Training Loss: 0.0000\n",
      "Epoch [20/50], Validation Loss: 0.0000\n",
      "Epoch [21/50], Training Loss: 0.0000\n",
      "Epoch [21/50], Validation Loss: 0.0000\n",
      "Epoch [22/50], Training Loss: 0.0000\n",
      "Epoch [22/50], Validation Loss: 0.0000\n",
      "Epoch [23/50], Training Loss: 0.0000\n",
      "Epoch [23/50], Validation Loss: 0.0000\n",
      "Epoch [24/50], Training Loss: 0.0000\n",
      "Epoch [24/50], Validation Loss: 0.0000\n",
      "Epoch [25/50], Training Loss: 0.0000\n",
      "Epoch [25/50], Validation Loss: 0.0000\n",
      "Epoch [26/50], Training Loss: 0.0000\n",
      "Epoch [26/50], Validation Loss: 0.0000\n",
      "Epoch [27/50], Training Loss: 0.0000\n",
      "Epoch [27/50], Validation Loss: 0.0000\n",
      "Epoch [28/50], Training Loss: 0.0000\n",
      "Epoch [28/50], Validation Loss: 0.0000\n",
      "Epoch [29/50], Training Loss: 0.0000\n",
      "Epoch [29/50], Validation Loss: 0.0000\n",
      "Epoch [30/50], Training Loss: 0.0000\n",
      "Epoch [30/50], Validation Loss: 0.0000\n",
      "Epoch [31/50], Training Loss: 0.0000\n",
      "Epoch [31/50], Validation Loss: 0.0000\n",
      "Epoch [32/50], Training Loss: 0.0000\n",
      "Epoch [32/50], Validation Loss: 0.0000\n",
      "Epoch [33/50], Training Loss: 0.0000\n",
      "Epoch [33/50], Validation Loss: 0.0000\n",
      "Epoch [34/50], Training Loss: 0.0000\n",
      "Epoch [34/50], Validation Loss: 0.0000\n",
      "Epoch [35/50], Training Loss: 0.0000\n",
      "Epoch [35/50], Validation Loss: 0.0000\n",
      "Epoch [36/50], Training Loss: 0.0000\n",
      "Epoch [36/50], Validation Loss: 0.0000\n",
      "Epoch [37/50], Training Loss: 0.0000\n",
      "Epoch [37/50], Validation Loss: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 117\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m train_encoder(encoder, train_loader, val_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 100\u001b[0m, in \u001b[0;36mtrain_encoder\u001b[0;34m(model, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m     98\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     99\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 100\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    101\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Encoder Model with 8 Convolutional Layers and latent space of size 1024\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First Convolution Block\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 224 -> 112\n",
    "\n",
    "            # Second Convolution Block\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 112 -> 56\n",
    "\n",
    "            # Third Convolution Block\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 56 -> 28\n",
    "\n",
    "            # Fourth Convolution Block\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 28 -> 14\n",
    "\n",
    "            # Fifth Convolution Block\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 14 -> 7\n",
    "\n",
    "            # Sixth Convolution Block\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            # No pooling to avoid shrinking the dimensions too much\n",
    "\n",
    "            # Seventh Convolution Block\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Eighth Convolution Block\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 7 -> 3 (final pooling)\n",
    "        )\n",
    "\n",
    "        # Fully connected layer to generate latent space of size 1024\n",
    "        self.fc = nn.Linear(512 * 3 * 3, 1024)  # Adapt to final size after convs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Data Loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Ensure consistent input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(root='Dataset/augmented_training', transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "val_data = datasets.ImageFolder(root='Dataset/validation', transform=transform)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training and Validation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder = Encoder().to(device)\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "def train_encoder(model, train_loader, val_loader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, _ in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, torch.zeros(outputs.size()).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, torch.zeros(outputs.size()).to(device))\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Start training\n",
    "train_encoder(encoder, train_loader, val_loader, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "096240f8-9254-43f3-bff5-266aa1ceb483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully loaded\n",
      "Epoch [1/50], Training Loss: 0.0005, Validation Loss: 0.0000\n",
      "Epoch [2/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [3/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [4/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [5/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [6/50], Training Loss: 0.0000, Validation Loss: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m encoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     54\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     56\u001b[0m     inputs, _ \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     57\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/datasets/folder.py:247\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/transforms/functional.py:127\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39munused\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_numpy_image\u001b[39m(img: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m}\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_tensor\u001b[39m(pic: Union[PILImage, np\u001b[38;5;241m.\u001b[39mndarray]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    This function does not support torchscript.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels=3, latent_dim=10):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 14 * 14, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "latent_dim = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(root='Dataset/augmented_training', transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_data = datasets.ImageFolder(root='Dataset/validation', transform=transform)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "print(\"Dataset successfully loaded\")\n",
    "\n",
    "encoder = Encoder(input_channels=3, latent_dim=latent_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, _ = batch\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        latent = encoder(inputs)\n",
    "        loss = criterion(latent, torch.zeros_like(latent))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    encoder.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, _ = batch\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            latent = encoder(inputs)\n",
    "            loss = criterion(latent, torch.zeros_like(latent))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Training Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "torch.save(encoder.state_dict(), 'encoder_model.pth')\n",
    "print(\"Model saved as 'encoder_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63b89961-e004-4c94-a4b9-a3199c252887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully loaded\n",
      "Epoch [1/50], Training Loss: 0.0018, Validation Loss: 0.0000\n",
      "Epoch [2/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [3/50], Training Loss: 8.9115, Validation Loss: 0.0588\n",
      "Epoch [4/50], Training Loss: 0.0258, Validation Loss: 0.0038\n",
      "Epoch [5/50], Training Loss: 0.0061, Validation Loss: 0.0063\n",
      "Epoch [6/50], Training Loss: 0.0041, Validation Loss: 0.0008\n",
      "Epoch [7/50], Training Loss: 28.4725, Validation Loss: 0.0635\n",
      "Epoch [8/50], Training Loss: 0.0460, Validation Loss: 0.0154\n",
      "Epoch [9/50], Training Loss: 0.0819, Validation Loss: 0.0037\n",
      "Epoch [10/50], Training Loss: 28.6809, Validation Loss: 2.8777\n",
      "Epoch [11/50], Training Loss: 7.4135, Validation Loss: 0.0906\n",
      "Epoch [12/50], Training Loss: 0.0461, Validation Loss: 0.1116\n",
      "Epoch [13/50], Training Loss: 506.8120, Validation Loss: 307.3792\n",
      "Epoch [14/50], Training Loss: 5.0674, Validation Loss: 0.6708\n",
      "Epoch [15/50], Training Loss: 2.3500, Validation Loss: 0.4614\n",
      "Epoch [16/50], Training Loss: 0.6360, Validation Loss: 1.0266\n",
      "Epoch [17/50], Training Loss: 11087.8572, Validation Loss: 29.0718\n",
      "Epoch [18/50], Training Loss: 17.0313, Validation Loss: 29.1272\n",
      "Epoch [19/50], Training Loss: 16.8959, Validation Loss: 83.8606\n",
      "Epoch [20/50], Training Loss: 8.6296, Validation Loss: 1.6022\n",
      "Epoch [21/50], Training Loss: 1.4035, Validation Loss: 0.3287\n",
      "Epoch [22/50], Training Loss: 0.4085, Validation Loss: 0.2889\n",
      "Epoch [23/50], Training Loss: 7899.5779, Validation Loss: 173.6688\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m     80\u001b[0m         inputs, _ \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     81\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/datasets/folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/datasets/folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torchvision/datasets/folder.py:264\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    263\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/PIL/Image.py:941\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    891\u001b[0m     mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m     colors: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image:\n\u001b[1;32m    897\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 941\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m    943\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    944\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    945\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/PIL/ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels=3, latent_dim=10):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),  # LeakyReLU instead of ReLU\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 14 * 14, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 10\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "train_data = datasets.ImageFolder(root='Dataset/augmented_training', transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_data = datasets.ImageFolder(root='Dataset/validation', transform=transform)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Dataset successfully loaded\")\n",
    "\n",
    "# Model, Loss function, and Optimizer\n",
    "encoder = Encoder(input_channels=3, latent_dim=latent_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, _ = batch\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        latent = encoder(inputs)\n",
    "        loss = criterion(latent, torch.zeros_like(latent))  # MSE with zero target\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation Phase\n",
    "    encoder.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, _ = batch\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            latent = encoder(inputs)\n",
    "            loss = criterion(latent, torch.zeros_like(latent))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Training Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(encoder.state_dict(), 'encoder_model.pth')\n",
    "print(\"Model saved as 'encoder_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ef1d31b-8d8a-49a6-87cd-2f1d17db541a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully loaded\n",
      "Epoch [1/50], Training Loss: 0.0002, Validation Loss: 0.0000\n",
      "Epoch [2/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [3/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [4/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [5/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [6/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [7/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [8/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [9/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [10/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [11/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [12/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [13/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [14/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [15/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [16/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [17/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [18/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [19/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [20/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [21/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [22/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [23/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [24/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [25/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [26/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [27/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [28/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [29/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [30/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [31/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [32/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [33/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [34/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [35/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [36/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [37/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [38/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [39/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [40/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [41/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [42/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [43/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [44/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [45/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [46/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [47/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [48/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [49/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch [50/50], Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Model saved as 'encoder_model.pth'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_channels=3, latent_dim=10):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 14 * 14, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, latent_dim)\n",
    "        )\n",
    "        self.apply(self._init_weights)  # Apply weight initialization\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 10\n",
    "learning_rate = 0.0001  # Reduced learning rate\n",
    "num_epochs = 50\n",
    "\n",
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Datasets and DataLoaders\n",
    "train_data = datasets.ImageFolder(root='Dataset/augmented_training', transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_data = datasets.ImageFolder(root='Dataset/validation', transform=transform)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Dataset successfully loaded\")\n",
    "\n",
    "# Model, Loss function, and Optimizer\n",
    "encoder = Encoder(input_channels=3, latent_dim=latent_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    encoder.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, _ = batch\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        latent = encoder(inputs)\n",
    "        loss = criterion(latent, torch.zeros_like(latent))  # MSE with zero target\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients to prevent gradient explosion\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # Validation Phase\n",
    "    encoder.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, _ = batch\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            latent = encoder(inputs)\n",
    "            loss = criterion(latent, torch.zeros_like(latent))\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Training Loss: {avg_train_loss:.4f}, \"\n",
    "          f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(encoder.state_dict(), 'encoder_model.pth')\n",
    "print(\"Model saved as 'encoder_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7b820a8-7575-4ed6-be9f-04d5e124936b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 0.002909\n",
      "Epoch [1/50], Validation Loss: 0.000001\n",
      "Epoch [2/50], Training Loss: 0.000002\n",
      "Epoch [2/50], Validation Loss: 0.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 120\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m train_encoder(encoder, train_loader, val_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 97\u001b[0m, in \u001b[0;36mtrain_encoder\u001b[0;34m(model, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m     95\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, _ \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 97\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     98\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     99\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Encoder Model with 8 Convolutional Layers and latent space of size 1024\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First Convolution Block\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 224 -> 112\n",
    "\n",
    "            # Second Convolution Block\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 112 -> 56\n",
    "\n",
    "            # Third Convolution Block\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 56 -> 28\n",
    "\n",
    "            # Fourth Convolution Block\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 28 -> 14\n",
    "\n",
    "            # Fifth Convolution Block\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 14 -> 7\n",
    "\n",
    "            # Sixth Convolution Block\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Seventh Convolution Block\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Eighth Convolution Block\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 7 -> 3 (final pooling)\n",
    "\n",
    "            # Dropout for regularization\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        # Fully connected layer to generate latent space of size 1024\n",
    "        self.fc = nn.Linear(512 * 3 * 3, 1024)  # Adjust to final size after conv layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Custom weight initialization function\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "# Data Loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Ensure consistent input size\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(root='Dataset/augmented_training', transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "val_data = datasets.ImageFolder(root='Dataset/validation', transform=transform)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training and Validation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder = Encoder().to(device)\n",
    "encoder.apply(init_weights)  # Apply weight initialization\n",
    "\n",
    "# Using a smaller learning rate and weight decay for regularization\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=1e-5, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()  # Keep MSELoss as per your request\n",
    "\n",
    "# Training Loop\n",
    "def train_encoder(model, train_loader, val_loader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, _ in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, torch.zeros(outputs.size()).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Training Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, torch.zeros(outputs.size()).to(device))\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {val_loss:.6f}\")\n",
    "\n",
    "# Start training\n",
    "train_encoder(encoder, train_loader, val_loader, epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7019345d-f171-4c47-8eea-06c3bd4a291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ws-008/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ws-008/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.015895, Validation Loss: 0.005262\n",
      "Epoch [2/50], Train Loss: 0.015692, Validation Loss: 0.004495\n",
      "Epoch [3/50], Train Loss: 0.007316, Validation Loss: 0.003436\n",
      "Epoch [4/50], Train Loss: 0.006209, Validation Loss: 0.002929\n",
      "Epoch [5/50], Train Loss: 0.005646, Validation Loss: 0.003540\n",
      "Epoch [6/50], Train Loss: 0.005366, Validation Loss: 0.002551\n",
      "Epoch [7/50], Train Loss: 0.005169, Validation Loss: 0.002349\n",
      "Epoch [8/50], Train Loss: 0.004983, Validation Loss: 0.002284\n",
      "Epoch [9/50], Train Loss: 0.004857, Validation Loss: 0.002161\n",
      "Epoch [10/50], Train Loss: 0.004760, Validation Loss: 0.002149\n",
      "Epoch [11/50], Train Loss: 0.004678, Validation Loss: 0.002155\n",
      "Epoch [12/50], Train Loss: 0.004612, Validation Loss: 0.002063\n",
      "Epoch [13/50], Train Loss: 0.004536, Validation Loss: 0.001976\n",
      "Epoch [14/50], Train Loss: 0.004474, Validation Loss: 0.001985\n",
      "Epoch [15/50], Train Loss: 0.004419, Validation Loss: 0.001913\n",
      "Epoch [16/50], Train Loss: 0.004377, Validation Loss: 0.001863\n",
      "Epoch [17/50], Train Loss: 0.004341, Validation Loss: 0.001894\n",
      "Epoch [18/50], Train Loss: 0.004311, Validation Loss: 0.001831\n",
      "Epoch [19/50], Train Loss: 0.004284, Validation Loss: 0.001820\n",
      "Epoch [20/50], Train Loss: 0.004261, Validation Loss: 0.001809\n",
      "Epoch [21/50], Train Loss: 0.004238, Validation Loss: 0.001794\n",
      "Epoch [22/50], Train Loss: 0.004219, Validation Loss: 0.001803\n",
      "Epoch [23/50], Train Loss: 0.004199, Validation Loss: 0.001782\n",
      "Epoch [24/50], Train Loss: 0.004179, Validation Loss: 0.001816\n",
      "Epoch [25/50], Train Loss: 0.004160, Validation Loss: 0.001768\n",
      "Epoch [26/50], Train Loss: 0.004140, Validation Loss: 0.001752\n",
      "Epoch [27/50], Train Loss: 0.004121, Validation Loss: 0.001747\n",
      "Epoch [28/50], Train Loss: 0.004100, Validation Loss: 0.001740\n",
      "Epoch [29/50], Train Loss: 0.004082, Validation Loss: 0.001952\n",
      "Epoch [30/50], Train Loss: 0.004068, Validation Loss: 0.001732\n",
      "Epoch [31/50], Train Loss: 0.004045, Validation Loss: 0.001706\n",
      "Epoch [32/50], Train Loss: 0.004028, Validation Loss: 0.001714\n",
      "Epoch [33/50], Train Loss: 0.004012, Validation Loss: 0.001707\n",
      "Epoch [34/50], Train Loss: 0.003995, Validation Loss: 0.001687\n",
      "Epoch [35/50], Train Loss: 0.003982, Validation Loss: 0.001671\n",
      "Epoch [36/50], Train Loss: 0.003966, Validation Loss: 0.001694\n",
      "Epoch [37/50], Train Loss: 0.003952, Validation Loss: 0.001672\n",
      "Epoch [38/50], Train Loss: 0.003938, Validation Loss: 0.001693\n",
      "Epoch [39/50], Train Loss: 0.003924, Validation Loss: 0.001665\n",
      "Epoch [40/50], Train Loss: 0.003912, Validation Loss: 0.001655\n",
      "Epoch [41/50], Train Loss: 0.003901, Validation Loss: 0.001648\n",
      "Epoch [42/50], Train Loss: 0.003888, Validation Loss: 0.001655\n",
      "Epoch [43/50], Train Loss: 0.003876, Validation Loss: 0.001631\n",
      "Epoch [44/50], Train Loss: 0.003865, Validation Loss: 0.001648\n",
      "Epoch [45/50], Train Loss: 0.003854, Validation Loss: 0.001641\n",
      "Epoch [46/50], Train Loss: 0.003843, Validation Loss: 0.001636\n",
      "Epoch [47/50], Train Loss: 0.003833, Validation Loss: 0.001622\n",
      "Epoch [48/50], Train Loss: 0.003823, Validation Loss: 0.001642\n",
      "Epoch [49/50], Train Loss: 0.003812, Validation Loss: 0.001610\n",
      "Epoch [50/50], Train Loss: 0.003803, Validation Loss: 0.001659\n",
      "Latent space and reconstructed images saved in 'output_images'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define transformations for training and validation data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = datasets.ImageFolder('Dataset/augmented_training', transform=transform)\n",
    "val_dataset = datasets.ImageFolder('Dataset/validation', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define Encoder using ResNet50 without fully connected layer\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, latent_space_size=1024):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        # Remove the fully connected layer\n",
    "        self.resnet = nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "        # Add linear layer for the latent space\n",
    "        self.fc = nn.Linear(2048, latent_space_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        latent = self.fc(x)\n",
    "        return latent\n",
    "\n",
    "# Define Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_space_size=1024):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_space_size, 2048 * 7 * 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2048, 1024, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # Use Sigmoid to normalize pixel values between [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), 2048, 7, 7)\n",
    "        reconstructed = self.decoder(x)\n",
    "        return reconstructed\n",
    "\n",
    "# Combine Encoder and Decoder into an Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_space_size=1024):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = ResNetEncoder(latent_space_size)\n",
    "        self.decoder = Decoder(latent_space_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return latent, reconstructed\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "latent_space_size = 1024\n",
    "autoencoder = Autoencoder(latent_space_size).to(device)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=50):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Training loop\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            latent, reconstructed = model(images)\n",
    "            loss = criterion(reconstructed, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, _ in val_loader:\n",
    "                images = images.to(device)\n",
    "                latent, reconstructed = model(images)\n",
    "                loss = criterion(reconstructed, images)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # Print losses\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Validation Loss: {val_loss:.6f}')\n",
    "\n",
    "# Train the model\n",
    "train_model(autoencoder, criterion, optimizer, train_loader, val_loader, num_epochs=50)\n",
    "\n",
    "# Function to save latent space and reconstructed images\n",
    "def save_latent_and_reconstructed(model, data_loader, output_dir='output_images', save_all=True):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(data_loader):\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Forward pass to get latent space and reconstructed images\n",
    "            latent, reconstructed = model(images)\n",
    "\n",
    "            # Save the latent space representations\n",
    "            latent_np = latent.cpu().numpy()\n",
    "            np.save(os.path.join(output_dir, f'latent_space_{i}.npy'), latent_np)\n",
    "\n",
    "            # Save reconstructed images\n",
    "            reconstructed_images = reconstructed.cpu().numpy().transpose(0, 2, 3, 1)  # Convert to (N, H, W, C)\n",
    "            reconstructed_images = np.clip(reconstructed_images, 0, 1)  # Ensure pixel values are within [0, 1]\n",
    "            \n",
    "            for j in range(reconstructed_images.shape[0]):\n",
    "                plt.imsave(os.path.join(output_dir, f'reconstructed_image_{i}_{j}.png'), reconstructed_images[j])\n",
    "\n",
    "    print(f\"Latent space and reconstructed images saved in '{output_dir}'\")\n",
    "\n",
    "# Save latent space and reconstructed images from the validation set\n",
    "save_latent_and_reconstructed(autoencoder, val_loader, output_dir='output_images', save_all=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65846ef6-1999-4b82-9bfd-accfbce32eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fff585-482e-4a93-9397-bf12516b4e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ab710-7b55-4048-8a1a-3814bfe3cd31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134a57f9-dbbf-469e-939f-a5fe09f85a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336ec50-c682-4a34-86e0-0b5a4f30d6ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410e615c-fa82-422e-95a2-2d3711264294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c3e36c-b5ee-4c1c-938e-b35dc7be85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "# Model definition\n",
    "class CBAMModel(nn.Module):\n",
    "    def __init__(self, latent_dim, cbam_block):\n",
    "        super(CBAMModel, self).__init__()\n",
    "        self.channel_adjust = nn.Conv2d(latent_dim, 3, kernel_size=1)\n",
    "        self.cbam = cbam_block\n",
    "\n",
    "    def forward(self, latent_space, reconstructed_images):\n",
    "        # Ensure both tensors have the same batch size\n",
    "        if latent_space.size(0) != reconstructed_images.size(0):\n",
    "            raise RuntimeError(\"Batch size mismatch between latent space and reconstructed images.\")\n",
    "        \n",
    "        # Resize latent space to match the spatial dimensions of reconstructed images\n",
    "        latent_space = F.interpolate(latent_space.unsqueeze(-1).unsqueeze(-1), size=reconstructed_images.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Adjust latent space channels to match image channels (3 for RGB)\n",
    "        latent_space = self.channel_adjust(latent_space)\n",
    "        \n",
    "        # Concatenate along the channel dimension\n",
    "        combined_input = torch.cat((latent_space, reconstructed_images), dim=1)\n",
    "        output = self.cbam(combined_input)\n",
    "        return output\n",
    "\n",
    "# CBAM block definition (simplified for demonstration)\n",
    "class CBAMBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(CBAMBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "# Load tensor or latent space\n",
    "def load_tensor_from_path(path, transform=None, is_latent=False):\n",
    "    try:\n",
    "        if is_latent:\n",
    "            # For latent space, use numpy.load and convert to a torch tensor\n",
    "            np_array = np.load(path)\n",
    "            tensor = torch.tensor(np_array, dtype=torch.float32)  # Convert to tensor\n",
    "        else:\n",
    "            image = read_image(path).float() / 255.0  # Normalize image\n",
    "            if transform:\n",
    "                tensor = transform(image)\n",
    "            else:\n",
    "                tensor = image\n",
    "        return tensor\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {path}: {e}\")\n",
    "        return None  # Return None if there is an error loading the file\n",
    "\n",
    "# Paths\n",
    "latent_space_path = 'latentspace'  # Path to latent space files (in .npy format)\n",
    "reconstructed_images_path = 'output_images'  # Path to reconstructed images\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load filenames\n",
    "latent_space_files = sorted(os.listdir(latent_space_path))\n",
    "reconstructed_image_files = sorted(os.listdir(reconstructed_images_path))\n",
    "\n",
    "# Check if paths have equal number of items\n",
    "assert len(latent_space_files) == len(reconstructed_image_files), \"Mismatch in latent and image files.\"\n",
    "\n",
    "# Initialize Model, Loss, Optimizer\n",
    "latent_dim = 1024  # Latent space size\n",
    "cbam_block = CBAMBlock(channels=6)  # 3 channels for latent, 3 for reconstructed, so 6 total channels\n",
    "model = CBAMModel(latent_dim, cbam_block).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training loop\n",
    "transform = transforms.Resize((224, 224))  # Resize images to 224x224 if needed\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i in range(len(latent_space_files)):\n",
    "        latent_file = latent_space_files[i]\n",
    "        recon_file = reconstructed_image_files[i]\n",
    "\n",
    "        latent_space = load_tensor_from_path(os.path.join(latent_space_path, latent_file), is_latent=True).unsqueeze(0).to(device)\n",
    "        reconstructed_images = load_tensor_from_path(os.path.join(reconstructed_images_path, recon_file), transform).unsqueeze(0).to(device)\n",
    "\n",
    "        # Skip if either tensor is None\n",
    "        if latent_space is None or reconstructed_images is None:\n",
    "            print(f\"Skipping pair {latent_file} and {recon_file} due to loading error.\")\n",
    "            continue\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(latent_space, reconstructed_images)\n",
    "        loss = criterion(output, reconstructed_images)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(latent_space_files):.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64480a3-fcf1-4d3d-80b5-53f50ea44519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ws-008/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ws-008/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.013823, Validation Loss: 0.004844\n",
      "Epoch [2/50], Train Loss: 0.008752, Validation Loss: 0.003465\n",
      "Epoch [3/50], Train Loss: 0.006174, Validation Loss: 0.003020\n",
      "Epoch [4/50], Train Loss: 0.005465, Validation Loss: 0.002523\n",
      "Epoch [5/50], Train Loss: 0.005082, Validation Loss: 0.002270\n",
      "Epoch [6/50], Train Loss: 0.004844, Validation Loss: 0.002116\n",
      "Epoch [7/50], Train Loss: 0.004671, Validation Loss: 0.002020\n",
      "Epoch [8/50], Train Loss: 0.004546, Validation Loss: 0.001950\n",
      "Epoch [9/50], Train Loss: 0.004449, Validation Loss: 0.001883\n",
      "Epoch [10/50], Train Loss: 0.004370, Validation Loss: 0.001842\n",
      "Epoch [11/50], Train Loss: 0.004308, Validation Loss: 0.001791\n",
      "Epoch [12/50], Train Loss: 0.004249, Validation Loss: 0.001795\n",
      "Epoch [13/50], Train Loss: 0.004202, Validation Loss: 0.001729\n",
      "Epoch [14/50], Train Loss: 0.004158, Validation Loss: 0.001737\n",
      "Epoch [15/50], Train Loss: 0.004121, Validation Loss: 0.001686\n",
      "Epoch [16/50], Train Loss: 0.004085, Validation Loss: 0.001686\n",
      "Epoch [17/50], Train Loss: 0.004054, Validation Loss: 0.001653\n",
      "Epoch [18/50], Train Loss: 0.004031, Validation Loss: 0.001631\n",
      "Epoch [19/50], Train Loss: 0.004004, Validation Loss: 0.001652\n",
      "Epoch [20/50], Train Loss: 0.003980, Validation Loss: 0.001596\n",
      "Epoch [21/50], Train Loss: 0.003961, Validation Loss: 0.001601\n",
      "Epoch [22/50], Train Loss: 0.003941, Validation Loss: 0.001578\n",
      "Epoch [23/50], Train Loss: 0.003923, Validation Loss: 0.001584\n",
      "Epoch [24/50], Train Loss: 0.003904, Validation Loss: 0.001580\n",
      "Epoch [25/50], Train Loss: 0.003889, Validation Loss: 0.001581\n",
      "Epoch [26/50], Train Loss: 0.003873, Validation Loss: 0.001586\n",
      "Epoch [27/50], Train Loss: 0.003859, Validation Loss: 0.001576\n",
      "Epoch [28/50], Train Loss: 0.003843, Validation Loss: 0.001560\n",
      "Epoch [29/50], Train Loss: 0.003828, Validation Loss: 0.001541\n",
      "Epoch [30/50], Train Loss: 0.003813, Validation Loss: 0.001545\n",
      "Epoch [31/50], Train Loss: 0.003800, Validation Loss: 0.001539\n",
      "Epoch [32/50], Train Loss: 0.003787, Validation Loss: 0.001541\n",
      "Epoch [33/50], Train Loss: 0.003773, Validation Loss: 0.001518\n",
      "Epoch [34/50], Train Loss: 0.003759, Validation Loss: 0.001521\n",
      "Epoch [35/50], Train Loss: 0.003749, Validation Loss: 0.001656\n",
      "Epoch [36/50], Train Loss: 0.003738, Validation Loss: 0.001503\n",
      "Epoch [37/50], Train Loss: 0.003723, Validation Loss: 0.001527\n",
      "Epoch [38/50], Train Loss: 0.003713, Validation Loss: 0.001506\n",
      "Epoch [39/50], Train Loss: 0.003702, Validation Loss: 0.001505\n",
      "Epoch [40/50], Train Loss: 0.003690, Validation Loss: 0.001500\n",
      "Epoch [41/50], Train Loss: 0.003680, Validation Loss: 0.001505\n",
      "Epoch [42/50], Train Loss: 0.003669, Validation Loss: 0.001523\n",
      "Epoch [43/50], Train Loss: 0.003662, Validation Loss: 0.001501\n",
      "Epoch [44/50], Train Loss: 0.003650, Validation Loss: 0.001514\n",
      "Epoch [45/50], Train Loss: 0.003640, Validation Loss: 0.001490\n",
      "Epoch [46/50], Train Loss: 0.003629, Validation Loss: 0.001548\n",
      "Epoch [47/50], Train Loss: 0.003620, Validation Loss: 0.001490\n",
      "Epoch [48/50], Train Loss: 0.003610, Validation Loss: 0.001502\n",
      "Epoch [49/50], Train Loss: 0.003601, Validation Loss: 0.001493\n",
      "Epoch [50/50], Train Loss: 0.003592, Validation Loss: 0.001513\n",
      "Latent space, labels, and reconstructed images saved in 'latent_space' and 'output_images'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define transformations for training and validation data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = datasets.ImageFolder('Dataset/augmented_training', transform=transform)\n",
    "val_dataset = datasets.ImageFolder('Dataset/validation', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define Encoder using ResNet50 without fully connected layer\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, latent_space_size=1024):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        # Remove the fully connected layer\n",
    "        self.resnet = nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "        # Add linear layer for the latent space\n",
    "        self.fc = nn.Linear(2048, latent_space_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        latent = self.fc(x)\n",
    "        return latent\n",
    "\n",
    "# Define Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_space_size=1024):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_space_size, 2048 * 7 * 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2048, 1024, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # Use Sigmoid to normalize pixel values between [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), 2048, 7, 7)\n",
    "        reconstructed = self.decoder(x)\n",
    "        return reconstructed\n",
    "\n",
    "# Combine Encoder and Decoder into an Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_space_size=1024):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = ResNetEncoder(latent_space_size)\n",
    "        self.decoder = Decoder(latent_space_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return latent, reconstructed\n",
    "\n",
    "# Instantiate model, loss function, and optimizer\n",
    "latent_space_size = 1024\n",
    "autoencoder = Autoencoder(latent_space_size).to(device)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=50):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        # Training loop\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            latent, reconstructed = model(images)\n",
    "            loss = criterion(reconstructed, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, _ in val_loader:\n",
    "                images = images.to(device)\n",
    "                latent, reconstructed = model(images)\n",
    "                loss = criterion(reconstructed, images)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # Print losses\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.6f}, Validation Loss: {val_loss:.6f}')\n",
    "\n",
    "# Train the model\n",
    "train_model(autoencoder, criterion, optimizer, train_loader, val_loader, num_epochs=50)\n",
    "\n",
    "# Function to save latent space, reconstructed images, and labels\n",
    "# Function to save latent space, reconstructed images, and labels\n",
    "def save_latent_and_reconstructed(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    latent_space_dir = 'latent_space'\n",
    "    output_images_dir = 'output_images'\n",
    "    os.makedirs(latent_space_dir, exist_ok=True)\n",
    "    os.makedirs(output_images_dir, exist_ok=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            latent, reconstructed = model(images)\n",
    "\n",
    "            # Save latent space and labels for each class\n",
    "            for i in range(images.size(0)):\n",
    "                class_label = train_dataset.classes[labels[i].item()]  # Get class name\n",
    "\n",
    "                # Save latent space\n",
    "                class_latent_dir = os.path.join(latent_space_dir, class_label)\n",
    "                os.makedirs(class_latent_dir, exist_ok=True)\n",
    "                latent_np = latent[i].cpu().numpy()\n",
    "                np.save(os.path.join(class_latent_dir, f'latent_{i}.npy'), latent_np)\n",
    "\n",
    "                # Save latent label\n",
    "                latent_label_np = np.array(labels[i].cpu().numpy())\n",
    "                np.save(os.path.join(class_latent_dir, f'label_{i}.npy'), latent_label_np)\n",
    "\n",
    "                # Save reconstructed images\n",
    "                class_dir = os.path.join(output_images_dir, class_label)\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                reconstructed_image = reconstructed[i].cpu().numpy().transpose(1, 2, 0)\n",
    "                reconstructed_image = np.clip(reconstructed_image, 0, 1)  # Ensure values are between 0 and 1\n",
    "                plt.imsave(os.path.join(class_dir, f'reconstructed_image_{i}.png'), reconstructed_image)\n",
    "\n",
    "    print(f\"Latent space, labels, and reconstructed images saved in '{latent_space_dir}' and '{output_images_dir}'\")\n",
    "\n",
    "\n",
    "\n",
    "# Save latent space, reconstructed images, and labels from the validation set\n",
    "save_latent_and_reconstructed(autoencoder, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb3afb-066f-4d16-8e40-d324f7e49dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156a11a9-4dae-4af4-be93-e9dda33dd5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2503c-2e4a-434f-98a5-e931e2c72cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c62a7ca0-e4ff-486e-b2cf-78c50f73f1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Reconstructed Images...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x64 and 512x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 146\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Train on reconstructed images\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on Reconstructed Images...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 146\u001b[0m train_model(reconstructed_loader, num_epochs)\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Fine-tuning on Latent Space\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-tuning on Latent Space...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 129\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(data_loader, num_epochs)\u001b[0m\n\u001b[1;32m    127\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    128\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 129\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m    130\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    131\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 74\u001b[0m, in \u001b[0;36mCBAMResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     73\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x)\n\u001b[0;32m---> 74\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcbam(x)\n\u001b[1;32m     75\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     76\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 54\u001b[0m, in \u001b[0;36mCBAM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 54\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcam(x)\n\u001b[1;32m     55\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msam(output)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m, in \u001b[0;36mCAM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m avg \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39madaptive_avg_pool2d(x, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m b, c, _, _ \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m---> 39\u001b[0m linear_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m.\u001b[39mview(b, c))\u001b[38;5;241m.\u001b[39mview(b, c, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m linear_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(avg\u001b[38;5;241m.\u001b[39mview(b, c))\u001b[38;5;241m.\u001b[39mview(b, c, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m output \u001b[38;5;241m=\u001b[39m linear_max \u001b[38;5;241m+\u001b[39m linear_avg\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x64 and 512x32)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# CBAM Components\n",
    "class SAM(nn.Module):\n",
    "    def __init__(self, bias=False):\n",
    "        super(SAM, self).__init__()\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3, bias=self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        max = torch.max(x, 1)[0].unsqueeze(1)\n",
    "        avg = torch.mean(x, 1).unsqueeze(1)\n",
    "        concat = torch.cat((max, avg), dim=1)\n",
    "        output = self.conv(concat)\n",
    "        output = torch.sigmoid(output) * x\n",
    "        return output\n",
    "\n",
    "class CAM(nn.Module):\n",
    "    def __init__(self, channels, r):\n",
    "        super(CAM, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.r = r\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=self.channels, out_features=self.channels // self.r, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=self.channels // self.r, out_features=self.channels, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        max = nn.functional.adaptive_max_pool2d(x, output_size=1)\n",
    "        avg = nn.functional.adaptive_avg_pool2d(x, output_size=1)\n",
    "        b, c, _, _ = x.size()\n",
    "        linear_max = self.linear(max.view(b, c)).view(b, c, 1, 1)\n",
    "        linear_avg = self.linear(avg.view(b, c)).view(b, c, 1, 1)\n",
    "        output = linear_max + linear_avg\n",
    "        output = torch.sigmoid(output) * x\n",
    "        return output\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, r):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.r = r\n",
    "        self.sam = SAM(bias=False)\n",
    "        self.cam = CAM(channels=self.channels, r=self.r)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.cam(x)\n",
    "        output = self.sam(output)\n",
    "        return output + x\n",
    "\n",
    "# CBAM ResNet Model\n",
    "class CBAMResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CBAMResNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            # ... Add your ResNet layers and CBAM layers here ...\n",
    "        )\n",
    "        self.cbam = CBAM(512, 16)  # Adjust based on your architecture\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.cbam(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Dataset class for Latent Space\n",
    "class LatentSpaceDataset(Dataset):\n",
    "    def __init__(self, latent_data, labels):\n",
    "        self.latent_data = latent_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latent_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.latent_data[idx], self.labels[idx]\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "num_classes = 10\n",
    "\n",
    "# DataLoader for Reconstructed Images\n",
    "reconstructed_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "reconstructed_dataset = datasets.ImageFolder(root='output_images2', transform=reconstructed_transform)\n",
    "reconstructed_loader = DataLoader(reconstructed_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load Latent Space Data (e.g., from a .npy file)\n",
    "latent_data = np.load('new_latent_space/0/latent_space.npy')  # Adjust the path\n",
    "latent_labels = np.load('new_latent_space/0/latent_labels.npy')  # Adjust the path\n",
    "latent_dataset = LatentSpaceDataset(latent_data, latent_labels)\n",
    "latent_loader = DataLoader(latent_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize Model, Loss Function, and Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CBAMResNet(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training function\n",
    "def train_model(data_loader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Print metrics\n",
    "        train_loss /= len(data_loader.dataset)\n",
    "        train_accuracy = accuracy_score(train_labels, train_preds)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# Train on reconstructed images\n",
    "print(\"Training on Reconstructed Images...\")\n",
    "train_model(reconstructed_loader, num_epochs)\n",
    "\n",
    "# Fine-tuning on Latent Space\n",
    "print(\"Fine-tuning on Latent Space...\")\n",
    "train_model(latent_loader, num_epochs)\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f32cd4ac-befe-4128-95fb-d2fe7c7b7e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 2/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 3/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 4/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 5/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 6/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 7/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 8/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 9/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 10/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 11/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 12/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 13/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 14/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 15/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 16/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 17/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 18/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 19/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 20/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 21/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 22/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 23/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 24/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 25/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 26/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 27/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 28/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 29/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 30/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 31/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 32/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 33/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 34/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 35/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 36/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 37/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 38/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 39/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 40/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 41/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 42/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 43/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 44/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 45/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 46/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 47/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 48/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 49/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 50/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Custom Dataset class to handle loading of latent spaces and labels\n",
    "class LatentSpaceDataset(Dataset):\n",
    "    def __init__(self, latent_space_dir):\n",
    "        self.latent_space_dir = latent_space_dir\n",
    "        self.latent_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Read the class folders (10 classes) from latent space directory\n",
    "        for class_folder in sorted(os.listdir(latent_space_dir)):\n",
    "            latent_class_folder = os.path.join(latent_space_dir, class_folder)\n",
    "\n",
    "            if os.path.isdir(latent_class_folder):\n",
    "                for latent_file in os.listdir(latent_class_folder):\n",
    "                    if latent_file.startswith(\"latent_\") and latent_file.endswith('.npy'):\n",
    "                        latent_path = os.path.join(latent_class_folder, latent_file)\n",
    "                        label_file = latent_file.replace(\"latent_\", \"label_\")  # Change filename to get the label\n",
    "                        label_path = os.path.join(latent_class_folder, label_file)\n",
    "\n",
    "                        if os.path.exists(label_path):  # Ensure the corresponding label exists\n",
    "                            self.latent_paths.append(latent_path)\n",
    "                            label_data = np.load(label_path)  # Load label data\n",
    "                            self.labels.append(label_data.item())  # Assuming labels are single values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.latent_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the latent space\n",
    "        latent_path = self.latent_paths[idx]\n",
    "        latent_space = np.load(latent_path)\n",
    "\n",
    "        # Get the label\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return latent_space, label\n",
    "\n",
    "# Load dataset\n",
    "latent_space_dir = 'latent_space'  # Path to latent spaces\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = LatentSpaceDataset(latent_space_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize SVM model\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 50\n",
    "\n",
    "# Train SVM on latent spaces for 50 epochs\n",
    "for epoch in range(num_epochs):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for latent_spaces, labels in dataloader:\n",
    "        latent_spaces_np = latent_spaces.numpy()\n",
    "        labels_np = labels.numpy()\n",
    "\n",
    "        # Train SVM on latent spaces\n",
    "        svm_model.fit(latent_spaces_np, labels_np)\n",
    "\n",
    "        # Predict on the same batch (or you can use a separate validation set)\n",
    "        preds = svm_model.predict(latent_spaces_np)\n",
    "\n",
    "        # Store predictions and true labels for metrics calculation\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels_np)\n",
    "\n",
    "    # Calculate metrics after each epoch\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Accuracy: {acc * 100:.2f}%')\n",
    "    print(f'Precision: {precision * 100:.2f}%')\n",
    "    print(f'Recall: {recall * 100:.2f}%')\n",
    "    print(f'F1 Score: {f1 * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddd67a25-eed7-45de-82ca-b7cc86b91803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 2/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 3/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 4/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 5/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 6/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 7/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 8/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 9/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 10/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 11/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 12/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 13/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 14/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 15/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n",
      "Epoch 16/50\n",
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1 Score: 100.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m svm_model\u001b[38;5;241m.\u001b[39mfit(images_np, labels_np)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Predict on the same batch\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m preds \u001b[38;5;241m=\u001b[39m svm_model\u001b[38;5;241m.\u001b[39mpredict(images_np)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Store predictions and true labels for metrics calculation\u001b[39;00m\n\u001b[1;32m     74\u001b[0m all_preds\u001b[38;5;241m.\u001b[39mextend(preds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:813\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    811\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 813\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:428\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    413\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_for_predict(X)\n\u001b[1;32m    429\u001b[0m     predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/svm/_base.py:606\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    603\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel):\n\u001b[0;32m--> 606\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    607\u001b[0m         X,\n\u001b[1;32m    608\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    609\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[1;32m    610\u001b[0m         order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    611\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    612\u001b[0m         reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    613\u001b[0m     )\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[1;32m    616\u001b[0m     X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py:994\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    983\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D input, got input with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    984\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    985\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         )\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;66;03m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[39;00m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;66;03m# to an error. This is needed because specifying a non complex\u001b[39;00m\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m# dtype to the function converts complex to real dtype,\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;66;03m# thereby passing the test made in the lines following the scope\u001b[39;00m\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;66;03m# of warnings context manager.\u001b[39;00m\n\u001b[0;32m--> 994\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[1;32m    995\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    996\u001b[0m             warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, ComplexWarning)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "# Custom Dataset class to handle loading of images and labels\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, output_images_dir):\n",
    "        self.output_images_dir = output_images_dir\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Read the class folders from the output images directory\n",
    "        for class_folder in sorted(os.listdir(output_images_dir)):\n",
    "            class_folder_path = os.path.join(output_images_dir, class_folder)\n",
    "\n",
    "            if os.path.isdir(class_folder_path):\n",
    "                for image_file in os.listdir(class_folder_path):\n",
    "                    if image_file.endswith('.png'):\n",
    "                        image_path = os.path.join(class_folder_path, image_file)\n",
    "                        self.image_paths.append(image_path)\n",
    "                        self.labels.append(class_folder)  # Use the folder name as the label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image_array = np.array(image)  # No resizing\n",
    "\n",
    "        # Get the label\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return image_array, label\n",
    "\n",
    "# Load dataset\n",
    "output_images_dir = 'output_images'  # Path to output images\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = ImageDataset(output_images_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize SVM model\n",
    "svm_model = SVC(kernel='linear')\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 50\n",
    "\n",
    "# Train SVM on reconstructed images for 50 epochs\n",
    "for epoch in range(num_epochs):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images_np = images.numpy().reshape(images.size(0), -1)  # Flatten the images\n",
    "\n",
    "        # Convert string labels to numerical labels\n",
    "        unique_labels = list(set(labels))  # Use the list of labels directly\n",
    "        label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        labels_np = np.array([label_mapping[label] for label in labels])\n",
    "\n",
    "        # Train SVM on images\n",
    "        svm_model.fit(images_np, labels_np)\n",
    "\n",
    "        # Predict on the same batch\n",
    "        preds = svm_model.predict(images_np)\n",
    "\n",
    "        # Store predictions and true labels for metrics calculation\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels_np)\n",
    "\n",
    "    # Calculate metrics after each epoch\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Accuracy: {acc * 100:.2f}%')\n",
    "    print(f'Precision: {precision * 100:.2f}%')\n",
    "    print(f'Recall: {recall * 100:.2f}%')\n",
    "    print(f'F1 Score: {f1 * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a713ac93-6896-43e7-95aa-4a058f6f44ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = datasets.ImageFolder('Dataset/augmented_training', transform=transform)\n",
    "val_dataset = datasets.ImageFolder('Dataset/validation', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "print('dataset successfully loaded')\n",
    "# Define Encoder\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, latent_space_size=1024):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.resnet = models.resnet50(weights='DEFAULT')  # Use updated weights parameter\n",
    "        self.resnet = nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "        self.fc = nn.Linear(2048, latent_space_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        latent = self.fc(x)\n",
    "        return latent\n",
    "\n",
    "# Instantiate the encoder\n",
    "encoder = ResNetEncoder().to(device)\n",
    "\n",
    "# Function to generate latent space for a dataset\n",
    "def generate_latent_space(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    latents = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, label in data_loader:\n",
    "            images = images.to(device)\n",
    "            latent = model(images)\n",
    "            latents.append(latent.cpu().numpy())\n",
    "            labels.extend(label.numpy())\n",
    "    \n",
    "    return np.concatenate(latents), np.array(labels)\n",
    "\n",
    "# Generate latent spaces for training and validation datasets\n",
    "train_latent, train_labels = generate_latent_space(encoder, train_loader)\n",
    "val_latent, val_labels = generate_latent_space(encoder, val_loader)\n",
    "\n",
    "# Train SVM model on the latent space\n",
    "svm_model = svm.SVC(kernel='linear', C=1.0)\n",
    "svm_model.fit(train_latent, train_labels)\n",
    "\n",
    "# Evaluate the SVM model on the validation set\n",
    "val_predictions = svm_model.predict(val_latent)\n",
    "\n",
    "# Calculate accuracy\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(val_labels, val_predictions, target_names=train_dataset.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "241404bf-7865-49ed-9f64-99a3da49e209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset successfully loaded\n",
      "Validation Accuracy: 0.8569\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Angioectasia       0.65      0.48      0.55       497\n",
      "        Bleeding       0.73      0.73      0.73       359\n",
      "         Erosion       0.48      0.51      0.50      1155\n",
      "        Erythema       0.40      0.32      0.35       297\n",
      "    Foreign Body       0.78      0.65      0.71       340\n",
      "Lymphangiectasia       0.66      0.54      0.59       343\n",
      "          Normal       0.93      0.95      0.94     12287\n",
      "           Polyp       0.47      0.40      0.43       500\n",
      "           Ulcer       0.90      0.84      0.87       286\n",
      "           Worms       0.98      0.88      0.93        68\n",
      "\n",
      "        accuracy                           0.86     16132\n",
      "       macro avg       0.70      0.63      0.66     16132\n",
      "    weighted avg       0.85      0.86      0.85     16132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = datasets.ImageFolder('Dataset/augmented_training', transform=transform)\n",
    "val_dataset = datasets.ImageFolder('Dataset/validation', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "print('dataset successfully loaded')\n",
    "# Define Encoder\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, latent_space_size=1024):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.resnet = models.resnet50(weights='DEFAULT')  # Use updated weights parameter\n",
    "        self.resnet = nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "        self.fc = nn.Linear(2048, latent_space_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        latent = self.fc(x)\n",
    "        return latent\n",
    "\n",
    "# Instantiate the encoder\n",
    "encoder = ResNetEncoder().to(device)\n",
    "\n",
    "# Function to generate latent space for a dataset\n",
    "def generate_latent_space(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    latents = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, label in data_loader:\n",
    "            images = images.to(device)\n",
    "            latent = model(images)\n",
    "            latents.append(latent.cpu().numpy())\n",
    "            labels.extend(label.numpy())\n",
    "    \n",
    "    return np.concatenate(latents), np.array(labels)\n",
    "\n",
    "# Generate latent spaces for training and validation datasets\n",
    "train_latent, train_labels = generate_latent_space(encoder, train_loader)\n",
    "val_latent, val_labels = generate_latent_space(encoder, val_loader)\n",
    "\n",
    "# Train SVM model on the latent space\n",
    "svm_model = svm.SVC(kernel='linear', C=1.0)\n",
    "svm_model.fit(train_latent, train_labels)\n",
    "\n",
    "# Evaluate the SVM model on the validation set\n",
    "val_predictions = svm_model.predict(val_latent)\n",
    "\n",
    "# Calculate accuracy\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(val_labels, val_predictions, target_names=train_dataset.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6df789-8ee7-4e0d-900a-a8d910e750a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully loaded\n",
      "Epoch [1/10], Loss: 2.2882, Validation Accuracy: 0.7617\n",
      "Epoch [2/10], Loss: 2.2657, Validation Accuracy: 0.7617\n",
      "Epoch [3/10], Loss: 2.2337, Validation Accuracy: 0.7617\n",
      "Epoch [4/10], Loss: 2.1848, Validation Accuracy: 0.7617\n",
      "Epoch [5/10], Loss: 2.1151, Validation Accuracy: 0.7617\n",
      "Epoch [6/10], Loss: 2.0301, Validation Accuracy: 0.7617\n",
      "Epoch [7/10], Loss: 1.9587, Validation Accuracy: 0.7617\n",
      "Epoch [8/10], Loss: 1.9200, Validation Accuracy: 0.7564\n",
      "Epoch [9/10], Loss: 1.8704, Validation Accuracy: 0.6835\n",
      "Epoch [10/10], Loss: 1.8293, Validation Accuracy: 0.6568\n",
      "Final Validation Accuracy: 0.6568\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Angioectasia       0.00      0.00      0.00       497\n",
      "        Bleeding       0.00      0.00      0.00       359\n",
      "         Erosion       0.00      0.00      0.00      1155\n",
      "        Erythema       0.06      0.80      0.11       297\n",
      "    Foreign Body       1.00      0.01      0.03       340\n",
      "Lymphangiectasia       0.00      0.00      0.00       343\n",
      "          Normal       0.89      0.84      0.86     12287\n",
      "           Polyp       0.07      0.03      0.05       500\n",
      "           Ulcer       0.00      0.00      0.00       286\n",
      "           Worms       0.26      0.43      0.32        68\n",
      "\n",
      "        accuracy                           0.66     16132\n",
      "       macro avg       0.23      0.21      0.14     16132\n",
      "    weighted avg       0.70      0.66      0.66     16132\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ws-008/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ws-008/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/ws-008/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = datasets.ImageFolder('Dataset/augmented_training', transform=transform)\n",
    "val_dataset = datasets.ImageFolder('Dataset/validation', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print('Dataset successfully loaded')\n",
    "\n",
    "# Define Encoder\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, latent_space_size=1024):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.resnet = models.resnet50(weights='DEFAULT')  # Updated weights parameter\n",
    "        self.resnet = nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "        self.fc = nn.Linear(2048, latent_space_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        latent = self.fc(x)\n",
    "        return latent\n",
    "\n",
    "# Instantiate the encoder\n",
    "encoder = ResNetEncoder().to(device)\n",
    "\n",
    "# Function to generate latent space for a dataset\n",
    "def generate_latent_space(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    latents = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, label in data_loader:\n",
    "            images = images.to(device)\n",
    "            latent = model(images)\n",
    "            latents.append(latent.cpu().numpy())\n",
    "            labels.extend(label.numpy())\n",
    "    \n",
    "    return np.concatenate(latents), np.array(labels)\n",
    "\n",
    "# Generate latent spaces for training and validation datasets\n",
    "train_latent, train_labels = generate_latent_space(encoder, train_loader)\n",
    "val_latent, val_labels = generate_latent_space(encoder, val_loader)\n",
    "\n",
    "# Define DNN\n",
    "class LatentSpaceDNN(nn.Module):\n",
    "    def __init__(self, input_size=1024, num_classes=10):\n",
    "        super(LatentSpaceDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate DNN model\n",
    "dnn_model = LatentSpaceDNN(input_size=1024, num_classes=len(train_dataset.classes)).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(dnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Function to train the DNN\n",
    "def train_dnn(model, train_latent, train_labels, val_latent, val_labels, num_epochs=10):\n",
    "    train_latent_tensor = torch.tensor(train_latent, dtype=torch.float32).to(device)\n",
    "    train_labels_tensor = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
    "\n",
    "    val_latent_tensor = torch.tensor(val_latent, dtype=torch.float32).to(device)\n",
    "    val_labels_tensor = torch.tensor(val_labels, dtype=torch.long).to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(train_latent_tensor)\n",
    "        loss = criterion(outputs, train_labels_tensor)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(val_latent_tensor)\n",
    "            _, val_predicted = torch.max(val_outputs, 1)\n",
    "            val_accuracy = (val_predicted == val_labels_tensor).float().mean().item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Train the DNN model\n",
    "train_dnn(dnn_model, train_latent, train_labels, val_latent, val_labels, num_epochs=10)\n",
    "\n",
    "# Final evaluation on validation set\n",
    "val_outputs = dnn_model(torch.tensor(val_latent, dtype=torch.float32).to(device))\n",
    "_, val_predicted = torch.max(val_outputs, 1)\n",
    "val_accuracy = (val_predicted.cpu().numpy() == val_labels).mean()\n",
    "\n",
    "print(f'Final Validation Accuracy: {val_accuracy:.4f}')\n",
    "print(classification_report(val_labels, val_predicted.cpu().numpy(), target_names=train_dataset.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e709dc4b-726f-4ee2-a644-a98e7f39fb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully loaded\n",
      "Epoch [1/50], Loss: 4267.1018, Validation Accuracy: 0.7517\n",
      "Epoch [2/50], Loss: 3282.1992, Validation Accuracy: 0.7892\n",
      "Epoch [3/50], Loss: 2985.2504, Validation Accuracy: 0.7771\n",
      "Epoch [4/50], Loss: 2789.3512, Validation Accuracy: 0.8180\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 139\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mprint\u001b[39m(classification_report(val_labels, val_predictions, target_names\u001b[38;5;241m=\u001b[39mtrain_dataset\u001b[38;5;241m.\u001b[39mclasses))\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m train_dnn(encoder, dnn_classifier, train_loader, val_loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 110\u001b[0m, in \u001b[0;36mtrain_dnn\u001b[0;34m(encoder, classifier, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization\u001b[39;00m\n\u001b[1;32m    109\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 110\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    111\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    113\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from collections import Counter\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = datasets.ImageFolder('Dataset/augmented_training', transform=transform)\n",
    "val_dataset = datasets.ImageFolder('Dataset/validation', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "print('Dataset successfully loaded')\n",
    "\n",
    "# Calculate class weights to handle class imbalance\n",
    "class_counts = Counter([label for _, label in train_dataset.imgs])\n",
    "class_weights = {cls: len(train_dataset) / count for cls, count in class_counts.items()}\n",
    "weights = [class_weights[label] for _, label in train_dataset.imgs]\n",
    "class_weights_tensor = torch.tensor([class_weights[i] for i in range(len(train_dataset.classes))]).to(device)\n",
    "\n",
    "# Define Encoder\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, latent_space_size=1024):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.resnet = models.resnet50(weights='DEFAULT')  # Use updated weights parameter\n",
    "        self.resnet = nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "        self.fc = nn.Linear(2048, latent_space_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        latent = self.fc(x)\n",
    "        return latent\n",
    "\n",
    "# Instantiate the encoder\n",
    "encoder = ResNetEncoder().to(device)\n",
    "\n",
    "# Define a simple DNN classifier on top of the latent space\n",
    "class DNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size=1024, num_classes=len(train_dataset.classes)):\n",
    "        super(DNNClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the classifier\n",
    "dnn_classifier = DNNClassifier().to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)  # Using class weights to handle imbalance\n",
    "optimizer = optim.Adam(dnn_classifier.parameters(), lr=0.0001)  # Lower learning rate\n",
    "\n",
    "# Function to generate latent space for a dataset\n",
    "def generate_latent_space(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    latents = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, label in data_loader:\n",
    "            images = images.to(device)\n",
    "            latent = model(images)\n",
    "            latents.append(latent.cpu().numpy())\n",
    "            labels.extend(label.numpy())\n",
    "    \n",
    "    return np.concatenate(latents), np.array(labels)\n",
    "\n",
    "# Function to train the DNN\n",
    "def train_dnn(encoder, classifier, train_loader, val_loader, num_epochs=50):\n",
    "    best_val_accuracy = 0\n",
    "    early_stopping_counter = 0\n",
    "    patience = 5\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        classifier.train()\n",
    "        total_loss = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Get latent space from encoder\n",
    "            latents = encoder(images)\n",
    "\n",
    "            # Forward pass through the classifier\n",
    "            outputs = classifier(latents)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation step\n",
    "        classifier.eval()\n",
    "        val_latents, val_labels = generate_latent_space(encoder, val_loader)\n",
    "        val_outputs = classifier(torch.tensor(val_latents).to(device))\n",
    "        val_predictions = torch.argmax(val_outputs, dim=1).cpu().numpy()\n",
    "\n",
    "        val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "    print(f'Final Validation Accuracy: {best_val_accuracy:.4f}')\n",
    "    print(classification_report(val_labels, val_predictions, target_names=train_dataset.classes))\n",
    "\n",
    "# Train the model\n",
    "train_dnn(encoder, dnn_classifier, train_loader, val_loader, num_epochs=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "337b664c-7ad2-4e41-b4d8-70ea5b8ca922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.1.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /home/ws-008/anaconda3/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/ws-008/anaconda3/lib/python3.12/site-packages (from xgboost) (2.20.5)\n",
      "Requirement already satisfied: scipy in /home/ws-008/anaconda3/lib/python3.12/site-packages (from xgboost) (1.14.1)\n",
      "Downloading xgboost-2.1.2-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.1.2\n",
      "Generating latent space for training data...\n",
      "Generating latent space for validation data...\n",
      "Training XGBoost classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ws-008/anaconda3/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [23:54:02] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating XGBoost model on validation data...\n",
      "Validation Accuracy: 0.8336\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Angioectasia       0.83      0.38      0.53       497\n",
      "        Bleeding       0.76      0.59      0.66       359\n",
      "         Erosion       0.44      0.49      0.47      1155\n",
      "        Erythema       0.37      0.25      0.30       297\n",
      "    Foreign Body       0.79      0.50      0.61       340\n",
      "Lymphangiectasia       0.62      0.37      0.46       343\n",
      "          Normal       0.90      0.95      0.93     12287\n",
      "           Polyp       0.39      0.30      0.34       500\n",
      "           Ulcer       0.87      0.58      0.70       286\n",
      "           Worms       0.79      0.96      0.87        68\n",
      "\n",
      "        accuracy                           0.83     16132\n",
      "       macro avg       0.68      0.54      0.59     16132\n",
      "    weighted avg       0.83      0.83      0.82     16132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define transformations for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load dataset using ImageFolder\n",
    "train_dataset = datasets.ImageFolder('Dataset/augmented_training', transform=transform)\n",
    "val_dataset = datasets.ImageFolder('Dataset/validation', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the Encoder using a pre-trained ResNet model\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, latent_space_size=1024):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.resnet = models.resnet50(weights='DEFAULT')  # Using ResNet50\n",
    "        self.resnet = nn.Sequential(*(list(self.resnet.children())[:-1]))  # Removing the classification layer\n",
    "        self.fc = nn.Linear(2048, latent_space_size)  # Fully connected layer to get latent space\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        latent = self.fc(x)  # Get the latent space\n",
    "        return latent\n",
    "\n",
    "# Instantiate the encoder and move it to the device (GPU/CPU)\n",
    "encoder = ResNetEncoder().to(device)\n",
    "\n",
    "# Function to generate latent spaces for a dataset\n",
    "def generate_latent_space(model, data_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    latents = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for images, label in data_loader:\n",
    "            images = images.to(device)\n",
    "            latent = model(images)  # Pass images through the encoder\n",
    "            latents.append(latent.cpu().numpy())  # Move latent space to CPU and store it\n",
    "            labels.extend(label.numpy())  # Store labels\n",
    "    \n",
    "    return np.concatenate(latents), np.array(labels)\n",
    "\n",
    "# Generate latent spaces for training and validation datasets\n",
    "print(\"Generating latent space for training data...\")\n",
    "train_latent, train_labels = generate_latent_space(encoder, train_loader)\n",
    "print(\"Generating latent space for validation data...\")\n",
    "val_latent, val_labels = generate_latent_space(encoder, val_loader)\n",
    "\n",
    "# Define and train the XGBoost model using the latent space\n",
    "print(\"Training XGBoost classifier...\")\n",
    "xgb_model = XGBClassifier(\n",
    "    objective='multi:softmax',  # Multi-class classification\n",
    "    num_class=len(train_dataset.classes),  # Number of classes\n",
    "    n_estimators=100,  # Number of trees\n",
    "    learning_rate=0.1,  # Learning rate\n",
    "    max_depth=6,  # Maximum depth of a tree\n",
    "    use_label_encoder=False  # Suppress warning about label encoding\n",
    ")\n",
    "\n",
    "# Fit the XGBoost model using latent spaces and corresponding labels\n",
    "xgb_model.fit(train_latent, train_labels)\n",
    "\n",
    "# Evaluate the model on validation latent space\n",
    "print(\"Evaluating XGBoost model on validation data...\")\n",
    "val_predictions = xgb_model.predict(val_latent)\n",
    "\n",
    "# Calculate accuracy\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "# Print detailed classification report\n",
    "print(classification_report(val_labels, val_predictions, target_names=train_dataset.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53ec2185-246a-4a17-8db4-9e058f018036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ws-008/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ws-008/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "==================================================\n",
      "Train Loss: 1.1129, Accuracy: 0.6283, Recall: 0.6283, Precision: 0.6178, F1 Score: 0.6179, Specificity: 0.5644\n",
      "Epoch 2/50\n",
      "==================================================\n",
      "Train Loss: 0.7984, Accuracy: 0.7300, Recall: 0.7300, Precision: 0.7269, F1 Score: 0.7260, Specificity: 0.6825\n",
      "Epoch 3/50\n",
      "==================================================\n",
      "Train Loss: 0.6737, Accuracy: 0.7699, Recall: 0.7699, Precision: 0.7681, F1 Score: 0.7676, Specificity: 0.7304\n",
      "Epoch 4/50\n",
      "==================================================\n",
      "Train Loss: 0.5949, Accuracy: 0.7963, Recall: 0.7963, Precision: 0.7950, F1 Score: 0.7946, Specificity: 0.7614\n",
      "Epoch 5/50\n",
      "==================================================\n",
      "Train Loss: 0.5334, Accuracy: 0.8166, Recall: 0.8166, Precision: 0.8156, F1 Score: 0.8153, Specificity: 0.7864\n",
      "Epoch 6/50\n",
      "==================================================\n",
      "Train Loss: 0.4870, Accuracy: 0.8318, Recall: 0.8318, Precision: 0.8302, F1 Score: 0.8304, Specificity: 0.8038\n",
      "Epoch 7/50\n",
      "==================================================\n",
      "Train Loss: 0.4429, Accuracy: 0.8470, Recall: 0.8470, Precision: 0.8458, F1 Score: 0.8460, Specificity: 0.8226\n",
      "Epoch 8/50\n",
      "==================================================\n",
      "Train Loss: 0.4118, Accuracy: 0.8572, Recall: 0.8572, Precision: 0.8561, F1 Score: 0.8563, Specificity: 0.8345\n",
      "Epoch 9/50\n",
      "==================================================\n",
      "Train Loss: 0.3869, Accuracy: 0.8658, Recall: 0.8658, Precision: 0.8647, F1 Score: 0.8650, Specificity: 0.8444\n",
      "Epoch 10/50\n",
      "==================================================\n",
      "Train Loss: 0.3622, Accuracy: 0.8759, Recall: 0.8759, Precision: 0.8750, F1 Score: 0.8752, Specificity: 0.8568\n",
      "Epoch 11/50\n",
      "==================================================\n",
      "Train Loss: 0.3470, Accuracy: 0.8794, Recall: 0.8794, Precision: 0.8785, F1 Score: 0.8788, Specificity: 0.8611\n",
      "Epoch 12/50\n",
      "==================================================\n",
      "Train Loss: 0.3257, Accuracy: 0.8868, Recall: 0.8868, Precision: 0.8861, F1 Score: 0.8863, Specificity: 0.8701\n",
      "Epoch 13/50\n",
      "==================================================\n",
      "Train Loss: 0.3146, Accuracy: 0.8915, Recall: 0.8915, Precision: 0.8907, F1 Score: 0.8909, Specificity: 0.8751\n",
      "Epoch 14/50\n",
      "==================================================\n",
      "Train Loss: 0.3034, Accuracy: 0.8952, Recall: 0.8952, Precision: 0.8944, F1 Score: 0.8947, Specificity: 0.8796\n",
      "Epoch 15/50\n",
      "==================================================\n",
      "Train Loss: 0.2925, Accuracy: 0.8985, Recall: 0.8985, Precision: 0.8980, F1 Score: 0.8981, Specificity: 0.8835\n",
      "Epoch 16/50\n",
      "==================================================\n",
      "Train Loss: 0.2819, Accuracy: 0.9031, Recall: 0.9031, Precision: 0.9024, F1 Score: 0.9026, Specificity: 0.8883\n",
      "Epoch 17/50\n",
      "==================================================\n",
      "Train Loss: 0.2749, Accuracy: 0.9047, Recall: 0.9047, Precision: 0.9043, F1 Score: 0.9044, Specificity: 0.8908\n",
      "Epoch 18/50\n",
      "==================================================\n",
      "Train Loss: 0.2609, Accuracy: 0.9094, Recall: 0.9094, Precision: 0.9089, F1 Score: 0.9090, Specificity: 0.8964\n",
      "Epoch 19/50\n",
      "==================================================\n",
      "Train Loss: 0.2559, Accuracy: 0.9114, Recall: 0.9114, Precision: 0.9110, F1 Score: 0.9111, Specificity: 0.8990\n",
      "Epoch 20/50\n",
      "==================================================\n",
      "Train Loss: 0.2475, Accuracy: 0.9136, Recall: 0.9136, Precision: 0.9132, F1 Score: 0.9133, Specificity: 0.9008\n",
      "Epoch 21/50\n",
      "==================================================\n",
      "Train Loss: 0.2390, Accuracy: 0.9172, Recall: 0.9172, Precision: 0.9168, F1 Score: 0.9169, Specificity: 0.9053\n",
      "Epoch 22/50\n",
      "==================================================\n",
      "Train Loss: 0.2392, Accuracy: 0.9169, Recall: 0.9169, Precision: 0.9166, F1 Score: 0.9167, Specificity: 0.9051\n",
      "Epoch 23/50\n",
      "==================================================\n",
      "Train Loss: 0.2311, Accuracy: 0.9204, Recall: 0.9204, Precision: 0.9201, F1 Score: 0.9202, Specificity: 0.9088\n",
      "Epoch 24/50\n",
      "==================================================\n",
      "Train Loss: 0.2286, Accuracy: 0.9206, Recall: 0.9206, Precision: 0.9202, F1 Score: 0.9203, Specificity: 0.9090\n",
      "Epoch 25/50\n",
      "==================================================\n",
      "Train Loss: 0.2251, Accuracy: 0.9224, Recall: 0.9224, Precision: 0.9221, F1 Score: 0.9222, Specificity: 0.9104\n",
      "Epoch 26/50\n",
      "==================================================\n",
      "Train Loss: 0.2174, Accuracy: 0.9240, Recall: 0.9240, Precision: 0.9237, F1 Score: 0.9238, Specificity: 0.9130\n",
      "Epoch 27/50\n",
      "==================================================\n",
      "Train Loss: 0.2149, Accuracy: 0.9248, Recall: 0.9248, Precision: 0.9245, F1 Score: 0.9246, Specificity: 0.9140\n",
      "Epoch 28/50\n",
      "==================================================\n",
      "Train Loss: 0.2124, Accuracy: 0.9257, Recall: 0.9257, Precision: 0.9254, F1 Score: 0.9255, Specificity: 0.9150\n",
      "Epoch 29/50\n",
      "==================================================\n",
      "Train Loss: 0.2039, Accuracy: 0.9283, Recall: 0.9283, Precision: 0.9280, F1 Score: 0.9281, Specificity: 0.9179\n",
      "Epoch 30/50\n",
      "==================================================\n",
      "Train Loss: 0.2040, Accuracy: 0.9277, Recall: 0.9277, Precision: 0.9273, F1 Score: 0.9274, Specificity: 0.9170\n",
      "Epoch 31/50\n",
      "==================================================\n",
      "Train Loss: 0.2013, Accuracy: 0.9297, Recall: 0.9297, Precision: 0.9295, F1 Score: 0.9295, Specificity: 0.9194\n",
      "Epoch 32/50\n",
      "==================================================\n",
      "Train Loss: 0.1963, Accuracy: 0.9314, Recall: 0.9314, Precision: 0.9312, F1 Score: 0.9312, Specificity: 0.9216\n",
      "Epoch 33/50\n",
      "==================================================\n",
      "Train Loss: 0.1923, Accuracy: 0.9328, Recall: 0.9328, Precision: 0.9326, F1 Score: 0.9327, Specificity: 0.9226\n",
      "Epoch 34/50\n",
      "==================================================\n",
      "Train Loss: 0.1864, Accuracy: 0.9345, Recall: 0.9345, Precision: 0.9343, F1 Score: 0.9343, Specificity: 0.9249\n",
      "Epoch 35/50\n",
      "==================================================\n",
      "Train Loss: 0.1860, Accuracy: 0.9351, Recall: 0.9351, Precision: 0.9349, F1 Score: 0.9350, Specificity: 0.9256\n",
      "Epoch 36/50\n",
      "==================================================\n",
      "Train Loss: 0.1819, Accuracy: 0.9355, Recall: 0.9355, Precision: 0.9353, F1 Score: 0.9354, Specificity: 0.9259\n",
      "Epoch 37/50\n",
      "==================================================\n",
      "Train Loss: 0.1780, Accuracy: 0.9370, Recall: 0.9370, Precision: 0.9367, F1 Score: 0.9368, Specificity: 0.9276\n",
      "Epoch 38/50\n",
      "==================================================\n",
      "Train Loss: 0.1803, Accuracy: 0.9362, Recall: 0.9362, Precision: 0.9360, F1 Score: 0.9360, Specificity: 0.9267\n",
      "Epoch 39/50\n",
      "==================================================\n",
      "Train Loss: 0.1751, Accuracy: 0.9381, Recall: 0.9381, Precision: 0.9379, F1 Score: 0.9379, Specificity: 0.9288\n",
      "Epoch 40/50\n",
      "==================================================\n",
      "Train Loss: 0.1721, Accuracy: 0.9396, Recall: 0.9396, Precision: 0.9394, F1 Score: 0.9395, Specificity: 0.9305\n",
      "Epoch 41/50\n",
      "==================================================\n",
      "Train Loss: 0.1702, Accuracy: 0.9400, Recall: 0.9400, Precision: 0.9398, F1 Score: 0.9398, Specificity: 0.9309\n",
      "Epoch 42/50\n",
      "==================================================\n",
      "Train Loss: 0.1672, Accuracy: 0.9401, Recall: 0.9401, Precision: 0.9399, F1 Score: 0.9400, Specificity: 0.9316\n",
      "Epoch 43/50\n",
      "==================================================\n",
      "Train Loss: 0.1647, Accuracy: 0.9418, Recall: 0.9418, Precision: 0.9416, F1 Score: 0.9417, Specificity: 0.9332\n",
      "Epoch 44/50\n",
      "==================================================\n",
      "Train Loss: 0.1632, Accuracy: 0.9411, Recall: 0.9411, Precision: 0.9410, F1 Score: 0.9410, Specificity: 0.9322\n",
      "Epoch 45/50\n",
      "==================================================\n",
      "Train Loss: 0.1620, Accuracy: 0.9422, Recall: 0.9422, Precision: 0.9420, F1 Score: 0.9421, Specificity: 0.9336\n",
      "Epoch 46/50\n",
      "==================================================\n",
      "Train Loss: 0.1585, Accuracy: 0.9447, Recall: 0.9447, Precision: 0.9446, F1 Score: 0.9446, Specificity: 0.9362\n",
      "Epoch 47/50\n",
      "==================================================\n",
      "Train Loss: 0.1604, Accuracy: 0.9424, Recall: 0.9424, Precision: 0.9423, F1 Score: 0.9423, Specificity: 0.9338\n",
      "Epoch 48/50\n",
      "==================================================\n",
      "Train Loss: 0.1577, Accuracy: 0.9441, Recall: 0.9441, Precision: 0.9440, F1 Score: 0.9440, Specificity: 0.9358\n",
      "Epoch 49/50\n",
      "==================================================\n",
      "Train Loss: 0.1551, Accuracy: 0.9447, Recall: 0.9447, Precision: 0.9446, F1 Score: 0.9446, Specificity: 0.9362\n",
      "Epoch 50/50\n",
      "==================================================\n",
      "Train Loss: 0.1560, Accuracy: 0.9452, Recall: 0.9452, Precision: 0.9450, F1 Score: 0.9451, Specificity: 0.9366\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "\n",
    "# CBAM Components\n",
    "class SAM(nn.Module):\n",
    "    def __init__(self, bias=False):\n",
    "        super(SAM, self).__init__()\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3, dilation=1, bias=self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        max = torch.max(x, 1)[0].unsqueeze(1)\n",
    "        avg = torch.mean(x, 1).unsqueeze(1)\n",
    "        concat = torch.cat((max, avg), dim=1)\n",
    "        output = self.conv(concat)\n",
    "        output = torch.sigmoid(output) * x\n",
    "        return output\n",
    "\n",
    "class CAM(nn.Module):\n",
    "    def __init__(self, channels, r):\n",
    "        super(CAM, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.r = r\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=self.channels, out_features=self.channels // self.r, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=self.channels // self.r, out_features=self.channels, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        max = F.adaptive_max_pool2d(x, output_size=1)\n",
    "        avg = F.adaptive_avg_pool2d(x, output_size=1)\n",
    "        b, c, _, _ = x.size()\n",
    "        linear_max = self.linear(max.view(b, c)).view(b, c, 1, 1)\n",
    "        linear_avg = self.linear(avg.view(b, c)).view(b, c, 1, 1)\n",
    "        output = linear_max + linear_avg\n",
    "        output = torch.sigmoid(output) * x\n",
    "        return output\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, r):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.r = r\n",
    "        self.sam = SAM(bias=False)\n",
    "        self.cam = CAM(channels=self.channels, r=self.r)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.cam(x)\n",
    "        output = self.sam(output)\n",
    "        return output + x\n",
    "\n",
    "# ResNet + CBAM Model\n",
    "class CBAMResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CBAMResNet, self).__init__()\n",
    "        self.features = models.resnet18(pretrained=True)\n",
    "        self.features.fc = nn.Identity()  # Remove the final fully connected layer\n",
    "        self.cbam = CBAM(512, 16)  # Add CBAM for the final feature map (before the pooling layer)\n",
    "        self.fc = nn.Linear(512, num_classes)  # Add a new fully connected layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features.conv1(x)\n",
    "        x = self.features.bn1(x)\n",
    "        x = self.features.relu(x)\n",
    "        x = self.features.maxpool(x)\n",
    "        \n",
    "        # Apply CBAM to intermediate feature maps\n",
    "        x = self.features.layer1(x)\n",
    "        x = self.features.layer2(x)\n",
    "        x = self.features.layer3(x)\n",
    "        x = self.features.layer4(x)\n",
    "        \n",
    "        x = self.cbam(x)  # Apply CBAM to the feature maps\n",
    "\n",
    "        x = self.features.avgpool(x)  # Global average pooling\n",
    "        x = torch.flatten(x, 1)  # Flatten for fully connected layer\n",
    "        x = self.fc(x)  # Final classification layer\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "num_classes = 10\n",
    "train_data_dir = 'Dataset/new_train'\n",
    "\n",
    "# Data Augmentation and Normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Datasets and Dataloaders\n",
    "train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CBAMResNet(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(preds, targets):\n",
    "    accuracy = accuracy_score(targets, preds)\n",
    "    recall = recall_score(targets, preds, average='weighted')\n",
    "    precision = precision_score(targets, preds, average='weighted')\n",
    "    f1 = f1_score(targets, preds, average='weighted')\n",
    "    \n",
    "    # Confusion matrix for specificity\n",
    "    cm = confusion_matrix(targets, preds)\n",
    "    tn = cm.diagonal()\n",
    "    fp = cm.sum(axis=1) - tn\n",
    "    specificity = tn / (tn + fp)\n",
    "    specificity = specificity.mean()  # Average across classes\n",
    "    \n",
    "    return accuracy, recall, precision, f1, specificity\n",
    "\n",
    "# Training Loop without checkpoints or validation\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Metrics\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_accuracy, train_recall, train_precision, train_f1, train_specificity = compute_metrics(train_preds, train_labels)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Recall: {train_recall:.4f}, Precision: {train_precision:.4f}, F1 Score: {train_f1:.4f}, Specificity: {train_specificity:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35b30fc0-01c9-48e6-815c-bf5a1b01cf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ws-008/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ws-008/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "==================================================\n",
      "Train Loss: 1.1068, Accuracy: 0.6304, Recall: 0.6304, Precision: 0.6196, F1 Score: 0.6198, Specificity: 0.5673\n",
      "Validation Loss: 0.7915, Accuracy: 0.7282, Recall: 0.7282, Precision: 0.7742, F1 Score: 0.7427, Specificity: 0.5803\n",
      "Checkpoint saved at checkpoint_epoch_1.pth\n",
      "Epoch 2/50\n",
      "==================================================\n",
      "Train Loss: 0.7937, Accuracy: 0.7314, Recall: 0.7314, Precision: 0.7275, F1 Score: 0.7272, Specificity: 0.6851\n",
      "Validation Loss: 0.7422, Accuracy: 0.7628, Recall: 0.7628, Precision: 0.8302, F1 Score: 0.7852, Specificity: 0.6706\n",
      "Checkpoint saved at checkpoint_epoch_2.pth\n",
      "Epoch 3/50\n",
      "==================================================\n",
      "Train Loss: 0.6734, Accuracy: 0.7691, Recall: 0.7691, Precision: 0.7670, F1 Score: 0.7666, Specificity: 0.7296\n",
      "Validation Loss: 0.4646, Accuracy: 0.8412, Recall: 0.8412, Precision: 0.8612, F1 Score: 0.8489, Specificity: 0.7080\n",
      "Checkpoint saved at checkpoint_epoch_3.pth\n",
      "Epoch 4/50\n",
      "==================================================\n",
      "Train Loss: 0.5919, Accuracy: 0.7962, Recall: 0.7962, Precision: 0.7948, F1 Score: 0.7945, Specificity: 0.7620\n",
      "Validation Loss: 0.4388, Accuracy: 0.8563, Recall: 0.8563, Precision: 0.8648, F1 Score: 0.8589, Specificity: 0.6927\n",
      "Checkpoint saved at checkpoint_epoch_4.pth\n",
      "Epoch 5/50\n",
      "==================================================\n",
      "Train Loss: 0.5338, Accuracy: 0.8167, Recall: 0.8167, Precision: 0.8148, F1 Score: 0.8151, Specificity: 0.7858\n",
      "Validation Loss: 0.4060, Accuracy: 0.8693, Recall: 0.8693, Precision: 0.8742, F1 Score: 0.8698, Specificity: 0.7152\n",
      "Checkpoint saved at checkpoint_epoch_5.pth\n",
      "Epoch 6/50\n",
      "==================================================\n",
      "Train Loss: 0.4822, Accuracy: 0.8332, Recall: 0.8332, Precision: 0.8320, F1 Score: 0.8321, Specificity: 0.8062\n",
      "Validation Loss: 0.5174, Accuracy: 0.8292, Recall: 0.8292, Precision: 0.8751, F1 Score: 0.8435, Specificity: 0.7655\n",
      "Checkpoint saved at checkpoint_epoch_6.pth\n",
      "Epoch 7/50\n",
      "==================================================\n",
      "Train Loss: 0.4475, Accuracy: 0.8455, Recall: 0.8455, Precision: 0.8442, F1 Score: 0.8444, Specificity: 0.8205\n",
      "Validation Loss: 0.4270, Accuracy: 0.8657, Recall: 0.8657, Precision: 0.8744, F1 Score: 0.8682, Specificity: 0.7385\n",
      "Checkpoint saved at checkpoint_epoch_7.pth\n",
      "Epoch 8/50\n",
      "==================================================\n",
      "Train Loss: 0.4123, Accuracy: 0.8587, Recall: 0.8587, Precision: 0.8577, F1 Score: 0.8579, Specificity: 0.8364\n",
      "Validation Loss: 0.4034, Accuracy: 0.8714, Recall: 0.8714, Precision: 0.8738, F1 Score: 0.8693, Specificity: 0.7230\n",
      "Checkpoint saved at checkpoint_epoch_8.pth\n",
      "Epoch 9/50\n",
      "==================================================\n",
      "Train Loss: 0.3866, Accuracy: 0.8663, Recall: 0.8663, Precision: 0.8653, F1 Score: 0.8656, Specificity: 0.8457\n",
      "Validation Loss: 0.3944, Accuracy: 0.8755, Recall: 0.8755, Precision: 0.8826, F1 Score: 0.8755, Specificity: 0.7408\n",
      "Checkpoint saved at checkpoint_epoch_9.pth\n",
      "Epoch 10/50\n",
      "==================================================\n",
      "Train Loss: 0.3649, Accuracy: 0.8735, Recall: 0.8735, Precision: 0.8728, F1 Score: 0.8729, Specificity: 0.8543\n",
      "Validation Loss: 0.4157, Accuracy: 0.8776, Recall: 0.8776, Precision: 0.8851, F1 Score: 0.8800, Specificity: 0.7569\n",
      "Checkpoint saved at checkpoint_epoch_10.pth\n",
      "Epoch 11/50\n",
      "==================================================\n",
      "Train Loss: 0.3418, Accuracy: 0.8816, Recall: 0.8816, Precision: 0.8809, F1 Score: 0.8811, Specificity: 0.8637\n",
      "Validation Loss: 0.3689, Accuracy: 0.8875, Recall: 0.8875, Precision: 0.8851, F1 Score: 0.8851, Specificity: 0.7289\n",
      "Checkpoint saved at checkpoint_epoch_11.pth\n",
      "Epoch 12/50\n",
      "==================================================\n",
      "Train Loss: 0.3297, Accuracy: 0.8871, Recall: 0.8871, Precision: 0.8863, F1 Score: 0.8865, Specificity: 0.8704\n",
      "Validation Loss: 0.3969, Accuracy: 0.8812, Recall: 0.8812, Precision: 0.8863, F1 Score: 0.8826, Specificity: 0.7561\n",
      "Checkpoint saved at checkpoint_epoch_12.pth\n",
      "Epoch 13/50\n",
      "==================================================\n",
      "Train Loss: 0.3155, Accuracy: 0.8904, Recall: 0.8904, Precision: 0.8897, F1 Score: 0.8899, Specificity: 0.8745\n",
      "Validation Loss: 0.3735, Accuracy: 0.8856, Recall: 0.8856, Precision: 0.8910, F1 Score: 0.8860, Specificity: 0.7729\n",
      "Checkpoint saved at checkpoint_epoch_13.pth\n",
      "Epoch 14/50\n",
      "==================================================\n",
      "Train Loss: 0.3046, Accuracy: 0.8959, Recall: 0.8959, Precision: 0.8953, F1 Score: 0.8955, Specificity: 0.8805\n",
      "Validation Loss: 0.3896, Accuracy: 0.8854, Recall: 0.8854, Precision: 0.8892, F1 Score: 0.8864, Specificity: 0.7437\n",
      "Checkpoint saved at checkpoint_epoch_14.pth\n",
      "Epoch 15/50\n",
      "==================================================\n",
      "Train Loss: 0.2905, Accuracy: 0.8986, Recall: 0.8986, Precision: 0.8980, F1 Score: 0.8982, Specificity: 0.8836\n",
      "Validation Loss: 0.3780, Accuracy: 0.8876, Recall: 0.8876, Precision: 0.8905, F1 Score: 0.8881, Specificity: 0.7545\n",
      "Checkpoint saved at checkpoint_epoch_15.pth\n",
      "Epoch 16/50\n",
      "==================================================\n",
      "Train Loss: 0.2781, Accuracy: 0.9034, Recall: 0.9034, Precision: 0.9028, F1 Score: 0.9030, Specificity: 0.8892\n",
      "Validation Loss: 0.3696, Accuracy: 0.8913, Recall: 0.8913, Precision: 0.8937, F1 Score: 0.8921, Specificity: 0.7574\n",
      "Checkpoint saved at checkpoint_epoch_16.pth\n",
      "Epoch 17/50\n",
      "==================================================\n",
      "Train Loss: 0.2696, Accuracy: 0.9077, Recall: 0.9077, Precision: 0.9071, F1 Score: 0.9072, Specificity: 0.8941\n",
      "Validation Loss: 0.3735, Accuracy: 0.8872, Recall: 0.8872, Precision: 0.8921, F1 Score: 0.8879, Specificity: 0.7538\n",
      "Checkpoint saved at checkpoint_epoch_17.pth\n",
      "Epoch 18/50\n",
      "==================================================\n",
      "Train Loss: 0.2590, Accuracy: 0.9093, Recall: 0.9093, Precision: 0.9088, F1 Score: 0.9089, Specificity: 0.8959\n",
      "Validation Loss: 0.3959, Accuracy: 0.8894, Recall: 0.8894, Precision: 0.8920, F1 Score: 0.8899, Specificity: 0.7652\n",
      "Checkpoint saved at checkpoint_epoch_18.pth\n",
      "Epoch 19/50\n",
      "==================================================\n",
      "Train Loss: 0.2530, Accuracy: 0.9128, Recall: 0.9128, Precision: 0.9124, F1 Score: 0.9125, Specificity: 0.9004\n",
      "Validation Loss: 0.3748, Accuracy: 0.8947, Recall: 0.8947, Precision: 0.8922, F1 Score: 0.8929, Specificity: 0.7526\n",
      "Checkpoint saved at checkpoint_epoch_19.pth\n",
      "Epoch 20/50\n",
      "==================================================\n",
      "Train Loss: 0.2481, Accuracy: 0.9136, Recall: 0.9136, Precision: 0.9131, F1 Score: 0.9133, Specificity: 0.9012\n",
      "Validation Loss: 0.3954, Accuracy: 0.8960, Recall: 0.8960, Precision: 0.8901, F1 Score: 0.8910, Specificity: 0.7450\n",
      "Checkpoint saved at checkpoint_epoch_20.pth\n",
      "Epoch 21/50\n",
      "==================================================\n",
      "Train Loss: 0.2392, Accuracy: 0.9176, Recall: 0.9176, Precision: 0.9172, F1 Score: 0.9173, Specificity: 0.9055\n",
      "Validation Loss: 0.4122, Accuracy: 0.8818, Recall: 0.8818, Precision: 0.8872, F1 Score: 0.8830, Specificity: 0.7567\n",
      "Checkpoint saved at checkpoint_epoch_21.pth\n",
      "Epoch 22/50\n",
      "==================================================\n",
      "Train Loss: 0.2365, Accuracy: 0.9185, Recall: 0.9185, Precision: 0.9181, F1 Score: 0.9182, Specificity: 0.9070\n",
      "Validation Loss: 0.3595, Accuracy: 0.8986, Recall: 0.8986, Precision: 0.8981, F1 Score: 0.8981, Specificity: 0.7464\n",
      "Checkpoint saved at checkpoint_epoch_22.pth\n",
      "Epoch 23/50\n",
      "==================================================\n",
      "Train Loss: 0.2277, Accuracy: 0.9205, Recall: 0.9205, Precision: 0.9202, F1 Score: 0.9203, Specificity: 0.9089\n",
      "Validation Loss: 0.4058, Accuracy: 0.8935, Recall: 0.8935, Precision: 0.8910, F1 Score: 0.8906, Specificity: 0.7389\n",
      "Checkpoint saved at checkpoint_epoch_23.pth\n",
      "Epoch 24/50\n",
      "==================================================\n",
      "Train Loss: 0.2255, Accuracy: 0.9207, Recall: 0.9207, Precision: 0.9204, F1 Score: 0.9204, Specificity: 0.9092\n",
      "Validation Loss: 0.3749, Accuracy: 0.8950, Recall: 0.8950, Precision: 0.8986, F1 Score: 0.8962, Specificity: 0.7720\n",
      "Checkpoint saved at checkpoint_epoch_24.pth\n",
      "Epoch 25/50\n",
      "==================================================\n",
      "Train Loss: 0.2202, Accuracy: 0.9230, Recall: 0.9230, Precision: 0.9226, F1 Score: 0.9227, Specificity: 0.9116\n",
      "Validation Loss: 0.3623, Accuracy: 0.8952, Recall: 0.8952, Precision: 0.8987, F1 Score: 0.8964, Specificity: 0.7629\n",
      "Checkpoint saved at checkpoint_epoch_25.pth\n",
      "Epoch 26/50\n",
      "==================================================\n",
      "Train Loss: 0.2122, Accuracy: 0.9260, Recall: 0.9260, Precision: 0.9258, F1 Score: 0.9259, Specificity: 0.9151\n",
      "Validation Loss: 0.3616, Accuracy: 0.9009, Recall: 0.9009, Precision: 0.9033, F1 Score: 0.9009, Specificity: 0.7837\n",
      "Checkpoint saved at checkpoint_epoch_26.pth\n",
      "Epoch 27/50\n",
      "==================================================\n",
      "Train Loss: 0.2093, Accuracy: 0.9271, Recall: 0.9271, Precision: 0.9268, F1 Score: 0.9269, Specificity: 0.9166\n",
      "Validation Loss: 0.3748, Accuracy: 0.8965, Recall: 0.8965, Precision: 0.9036, F1 Score: 0.8991, Specificity: 0.7931\n",
      "Checkpoint saved at checkpoint_epoch_27.pth\n",
      "Epoch 28/50\n",
      "==================================================\n",
      "Train Loss: 0.2082, Accuracy: 0.9276, Recall: 0.9276, Precision: 0.9274, F1 Score: 0.9274, Specificity: 0.9171\n",
      "Validation Loss: 0.4031, Accuracy: 0.8905, Recall: 0.8905, Precision: 0.8962, F1 Score: 0.8913, Specificity: 0.7771\n",
      "Checkpoint saved at checkpoint_epoch_28.pth\n",
      "Epoch 29/50\n",
      "==================================================\n",
      "Train Loss: 0.2044, Accuracy: 0.9290, Recall: 0.9290, Precision: 0.9287, F1 Score: 0.9288, Specificity: 0.9187\n",
      "Validation Loss: 0.3967, Accuracy: 0.8848, Recall: 0.8848, Precision: 0.8943, F1 Score: 0.8879, Specificity: 0.7759\n",
      "Checkpoint saved at checkpoint_epoch_29.pth\n",
      "Epoch 30/50\n",
      "==================================================\n",
      "Train Loss: 0.1957, Accuracy: 0.9313, Recall: 0.9313, Precision: 0.9310, F1 Score: 0.9311, Specificity: 0.9213\n",
      "Validation Loss: 0.4016, Accuracy: 0.8962, Recall: 0.8962, Precision: 0.8995, F1 Score: 0.8972, Specificity: 0.7796\n",
      "Checkpoint saved at checkpoint_epoch_30.pth\n",
      "Epoch 31/50\n",
      "==================================================\n",
      "Train Loss: 0.1947, Accuracy: 0.9323, Recall: 0.9323, Precision: 0.9321, F1 Score: 0.9321, Specificity: 0.9226\n",
      "Validation Loss: 0.4131, Accuracy: 0.8907, Recall: 0.8907, Precision: 0.8965, F1 Score: 0.8922, Specificity: 0.7760\n",
      "Checkpoint saved at checkpoint_epoch_31.pth\n",
      "Epoch 32/50\n",
      "==================================================\n",
      "Train Loss: 0.1905, Accuracy: 0.9326, Recall: 0.9326, Precision: 0.9324, F1 Score: 0.9324, Specificity: 0.9224\n",
      "Validation Loss: 0.3864, Accuracy: 0.8980, Recall: 0.8980, Precision: 0.9032, F1 Score: 0.8997, Specificity: 0.7888\n",
      "Checkpoint saved at checkpoint_epoch_32.pth\n",
      "Epoch 33/50\n",
      "==================================================\n",
      "Train Loss: 0.1871, Accuracy: 0.9344, Recall: 0.9344, Precision: 0.9342, F1 Score: 0.9343, Specificity: 0.9247\n",
      "Validation Loss: 0.3723, Accuracy: 0.9019, Recall: 0.9019, Precision: 0.9031, F1 Score: 0.9019, Specificity: 0.7850\n",
      "Checkpoint saved at checkpoint_epoch_33.pth\n",
      "Epoch 34/50\n",
      "==================================================\n",
      "Train Loss: 0.1842, Accuracy: 0.9348, Recall: 0.9348, Precision: 0.9346, F1 Score: 0.9346, Specificity: 0.9253\n",
      "Validation Loss: 0.3832, Accuracy: 0.9011, Recall: 0.9011, Precision: 0.8989, F1 Score: 0.8991, Specificity: 0.7638\n",
      "Checkpoint saved at checkpoint_epoch_34.pth\n",
      "Epoch 35/50\n",
      "==================================================\n",
      "Train Loss: 0.1844, Accuracy: 0.9343, Recall: 0.9343, Precision: 0.9340, F1 Score: 0.9341, Specificity: 0.9246\n",
      "Validation Loss: 0.3844, Accuracy: 0.8962, Recall: 0.8962, Precision: 0.8988, F1 Score: 0.8973, Specificity: 0.7729\n",
      "Checkpoint saved at checkpoint_epoch_35.pth\n",
      "Epoch 36/50\n",
      "==================================================\n",
      "Train Loss: 0.1803, Accuracy: 0.9366, Recall: 0.9366, Precision: 0.9364, F1 Score: 0.9365, Specificity: 0.9271\n",
      "Validation Loss: 0.4016, Accuracy: 0.8952, Recall: 0.8952, Precision: 0.8989, F1 Score: 0.8967, Specificity: 0.7703\n",
      "Checkpoint saved at checkpoint_epoch_36.pth\n",
      "Epoch 37/50\n",
      "==================================================\n",
      "Train Loss: 0.1790, Accuracy: 0.9367, Recall: 0.9367, Precision: 0.9365, F1 Score: 0.9365, Specificity: 0.9273\n",
      "Validation Loss: 0.4135, Accuracy: 0.9012, Recall: 0.9012, Precision: 0.8963, F1 Score: 0.8974, Specificity: 0.7466\n",
      "Checkpoint saved at checkpoint_epoch_37.pth\n",
      "Epoch 38/50\n",
      "==================================================\n",
      "Train Loss: 0.1727, Accuracy: 0.9394, Recall: 0.9394, Precision: 0.9393, F1 Score: 0.9393, Specificity: 0.9302\n",
      "Validation Loss: 0.4050, Accuracy: 0.9001, Recall: 0.9001, Precision: 0.9002, F1 Score: 0.8996, Specificity: 0.7699\n",
      "Checkpoint saved at checkpoint_epoch_38.pth\n",
      "Epoch 39/50\n",
      "==================================================\n",
      "Train Loss: 0.1743, Accuracy: 0.9381, Recall: 0.9381, Precision: 0.9379, F1 Score: 0.9380, Specificity: 0.9288\n",
      "Validation Loss: 0.3848, Accuracy: 0.9026, Recall: 0.9026, Precision: 0.9055, F1 Score: 0.9033, Specificity: 0.7838\n",
      "Checkpoint saved at checkpoint_epoch_39.pth\n",
      "Epoch 40/50\n",
      "==================================================\n",
      "Train Loss: 0.1681, Accuracy: 0.9409, Recall: 0.9409, Precision: 0.9407, F1 Score: 0.9408, Specificity: 0.9321\n",
      "Validation Loss: 0.3863, Accuracy: 0.9038, Recall: 0.9038, Precision: 0.9033, F1 Score: 0.9033, Specificity: 0.7725\n",
      "Checkpoint saved at checkpoint_epoch_40.pth\n",
      "Epoch 41/50\n",
      "==================================================\n",
      "Train Loss: 0.1696, Accuracy: 0.9399, Recall: 0.9399, Precision: 0.9397, F1 Score: 0.9397, Specificity: 0.9309\n",
      "Validation Loss: 0.3729, Accuracy: 0.9021, Recall: 0.9021, Precision: 0.9028, F1 Score: 0.9021, Specificity: 0.7709\n",
      "Checkpoint saved at checkpoint_epoch_41.pth\n",
      "Epoch 42/50\n",
      "==================================================\n",
      "Train Loss: 0.1686, Accuracy: 0.9404, Recall: 0.9404, Precision: 0.9402, F1 Score: 0.9402, Specificity: 0.9313\n",
      "Validation Loss: 0.3972, Accuracy: 0.8978, Recall: 0.8978, Precision: 0.8994, F1 Score: 0.8980, Specificity: 0.7741\n",
      "Checkpoint saved at checkpoint_epoch_42.pth\n",
      "Epoch 43/50\n",
      "==================================================\n",
      "Train Loss: 0.1645, Accuracy: 0.9415, Recall: 0.9415, Precision: 0.9413, F1 Score: 0.9414, Specificity: 0.9330\n",
      "Validation Loss: 0.4061, Accuracy: 0.9008, Recall: 0.9008, Precision: 0.8993, F1 Score: 0.8996, Specificity: 0.7719\n",
      "Checkpoint saved at checkpoint_epoch_43.pth\n",
      "Epoch 44/50\n",
      "==================================================\n",
      "Train Loss: 0.1641, Accuracy: 0.9422, Recall: 0.9422, Precision: 0.9420, F1 Score: 0.9420, Specificity: 0.9338\n",
      "Validation Loss: 0.4479, Accuracy: 0.8981, Recall: 0.8981, Precision: 0.8944, F1 Score: 0.8958, Specificity: 0.7598\n",
      "Checkpoint saved at checkpoint_epoch_44.pth\n",
      "Epoch 45/50\n",
      "==================================================\n",
      "Train Loss: 0.1584, Accuracy: 0.9438, Recall: 0.9438, Precision: 0.9436, F1 Score: 0.9437, Specificity: 0.9353\n",
      "Validation Loss: 0.4115, Accuracy: 0.8988, Recall: 0.8988, Precision: 0.9019, F1 Score: 0.8987, Specificity: 0.7787\n",
      "Checkpoint saved at checkpoint_epoch_45.pth\n",
      "Epoch 46/50\n",
      "==================================================\n",
      "Train Loss: 0.1573, Accuracy: 0.9434, Recall: 0.9434, Precision: 0.9432, F1 Score: 0.9432, Specificity: 0.9346\n",
      "Validation Loss: 0.4014, Accuracy: 0.9024, Recall: 0.9024, Precision: 0.9029, F1 Score: 0.9023, Specificity: 0.7803\n",
      "Checkpoint saved at checkpoint_epoch_46.pth\n",
      "Epoch 47/50\n",
      "==================================================\n",
      "Train Loss: 0.1577, Accuracy: 0.9442, Recall: 0.9442, Precision: 0.9441, F1 Score: 0.9441, Specificity: 0.9359\n",
      "Validation Loss: 0.4027, Accuracy: 0.9022, Recall: 0.9022, Precision: 0.9051, F1 Score: 0.9034, Specificity: 0.7834\n",
      "Checkpoint saved at checkpoint_epoch_47.pth\n",
      "Epoch 48/50\n",
      "==================================================\n",
      "Train Loss: 0.1560, Accuracy: 0.9441, Recall: 0.9441, Precision: 0.9439, F1 Score: 0.9439, Specificity: 0.9359\n",
      "Validation Loss: 0.4025, Accuracy: 0.8972, Recall: 0.8972, Precision: 0.9008, F1 Score: 0.8984, Specificity: 0.7824\n",
      "Checkpoint saved at checkpoint_epoch_48.pth\n",
      "Epoch 49/50\n",
      "==================================================\n",
      "Train Loss: 0.1552, Accuracy: 0.9452, Recall: 0.9452, Precision: 0.9450, F1 Score: 0.9450, Specificity: 0.9369\n",
      "Validation Loss: 0.3962, Accuracy: 0.9055, Recall: 0.9055, Precision: 0.9065, F1 Score: 0.9054, Specificity: 0.7831\n",
      "Checkpoint saved at checkpoint_epoch_49.pth\n",
      "Epoch 50/50\n",
      "==================================================\n",
      "Train Loss: 0.1507, Accuracy: 0.9469, Recall: 0.9469, Precision: 0.9467, F1 Score: 0.9468, Specificity: 0.9387\n",
      "Validation Loss: 0.4182, Accuracy: 0.9025, Recall: 0.9025, Precision: 0.9059, F1 Score: 0.9038, Specificity: 0.7881\n",
      "Checkpoint saved at checkpoint_epoch_50.pth\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "\n",
    "# CBAM Components\n",
    "class SAM(nn.Module):\n",
    "    def __init__(self, bias=False):\n",
    "        super(SAM, self).__init__()\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3, dilation=1, bias=self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        max = torch.max(x, 1)[0].unsqueeze(1)\n",
    "        avg = torch.mean(x, 1).unsqueeze(1)\n",
    "        concat = torch.cat((max, avg), dim=1)\n",
    "        output = self.conv(concat)\n",
    "        output = torch.sigmoid(output) * x\n",
    "        return output\n",
    "\n",
    "class CAM(nn.Module):\n",
    "    def __init__(self, channels, r):\n",
    "        super(CAM, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.r = r\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=self.channels, out_features=self.channels // self.r, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=self.channels // self.r, out_features=self.channels, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        max = F.adaptive_max_pool2d(x, output_size=1)\n",
    "        avg = F.adaptive_avg_pool2d(x, output_size=1)\n",
    "        b, c, _, _ = x.size()\n",
    "        linear_max = self.linear(max.view(b, c)).view(b, c, 1, 1)\n",
    "        linear_avg = self.linear(avg.view(b, c)).view(b, c, 1, 1)\n",
    "        output = linear_max + linear_avg\n",
    "        output = torch.sigmoid(output) * x\n",
    "        return output\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, r):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.r = r\n",
    "        self.sam = SAM(bias=False)\n",
    "        self.cam = CAM(channels=self.channels, r=self.r)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.cam(x)\n",
    "        output = self.sam(output)\n",
    "        return output + x\n",
    "\n",
    "# ResNet + CBAM Model\n",
    "class CBAMResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CBAMResNet, self).__init__()\n",
    "        self.features = models.resnet18(pretrained=True)\n",
    "        self.features.fc = nn.Identity()  # Remove the final fully connected layer\n",
    "        self.cbam = CBAM(512, 16)  # Add CBAM for the final feature map (before the pooling layer)\n",
    "        self.fc = nn.Linear(512, num_classes)  # Add a new fully connected layer for classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features.conv1(x)\n",
    "        x = self.features.bn1(x)\n",
    "        x = self.features.relu(x)\n",
    "        x = self.features.maxpool(x)\n",
    "        \n",
    "        # Apply CBAM to intermediate feature maps\n",
    "        x = self.features.layer1(x)\n",
    "        x = self.features.layer2(x)\n",
    "        x = self.features.layer3(x)\n",
    "        x = self.features.layer4(x)\n",
    "        \n",
    "        x = self.cbam(x)  # Apply CBAM to the feature maps\n",
    "\n",
    "        x = self.features.avgpool(x)  # Global average pooling\n",
    "        x = torch.flatten(x, 1)  # Flatten for fully connected layer\n",
    "        x = self.fc(x)  # Final classification layer\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "num_classes = 10\n",
    "train_data_dir = 'Dataset/new_train'\n",
    "val_data_dir = 'Dataset/validation'\n",
    "\n",
    "# Data Augmentation and Normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Datasets and Dataloaders\n",
    "train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_data_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model, Loss, and Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CBAMResNet(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(preds, targets):\n",
    "    accuracy = accuracy_score(targets, preds)\n",
    "    recall = recall_score(targets, preds, average='weighted')\n",
    "    precision = precision_score(targets, preds, average='weighted')\n",
    "    f1 = f1_score(targets, preds, average='weighted')\n",
    "    \n",
    "    # Confusion matrix for specificity\n",
    "    cm = confusion_matrix(targets, preds)\n",
    "    tn = cm.diagonal()\n",
    "    fp = cm.sum(axis=1) - tn\n",
    "    specificity = tn / (tn + fp)\n",
    "    specificity = specificity.mean()  # Average across classes\n",
    "    \n",
    "    return accuracy, recall, precision, f1, specificity\n",
    "\n",
    "# Training and Evaluation Loop with Checkpoints\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        train_preds.extend(preds.cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Metrics\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_accuracy, train_recall, train_precision, train_f1, train_specificity = compute_metrics(train_preds, train_labels)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Recall: {train_recall:.4f}, Precision: {train_precision:.4f}, F1 Score: {train_f1:.4f}, Specificity: {train_specificity:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Metrics\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy, val_recall, val_precision, val_f1, val_specificity = compute_metrics(val_preds, val_labels)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Recall: {val_recall:.4f}, Precision: {val_precision:.4f}, F1 Score: {val_f1:.4f}, Specificity: {val_specificity:.4f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_path = f\"checkpoint_epoch_{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c298834a-e1a2-4ebc-91b2-4fe38115d5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully loaded\n",
      "Generating latent space for training data...\n",
      "Generating latent space for validation data...\n",
      "Training SVM...\n",
      "Training KNN...\n",
      "Training Random Forest...\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ws-008/anaconda3/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [03:35:24] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ensemble with hard voting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ws-008/anaconda3/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [04:52:01] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting on validation data...\n",
      "Validation Accuracy (Hard Voting Ensemble): 0.8580\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "    Angioectasia       0.79      0.44      0.57       497\n",
      "        Bleeding       0.75      0.69      0.72       359\n",
      "         Erosion       0.46      0.59      0.52      1155\n",
      "        Erythema       0.45      0.27      0.34       297\n",
      "    Foreign Body       0.90      0.65      0.75       340\n",
      "Lymphangiectasia       0.80      0.48      0.60       343\n",
      "          Normal       0.92      0.96      0.94     12287\n",
      "           Polyp       0.53      0.31      0.39       500\n",
      "           Ulcer       0.97      0.64      0.77       286\n",
      "           Worms       0.91      0.99      0.94        68\n",
      "\n",
      "        accuracy                           0.86     16132\n",
      "       macro avg       0.75      0.60      0.65     16132\n",
      "    weighted avg       0.86      0.86      0.85     16132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = datasets.ImageFolder('Dataset/augmented_training', transform=transform)\n",
    "val_dataset = datasets.ImageFolder('Dataset/validation', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "print('Dataset successfully loaded')\n",
    "\n",
    "# Define Encoder (ResNet50)\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, latent_space_size=1024):\n",
    "        super(ResNetEncoder, self).__init__()\n",
    "        self.resnet = models.resnet50(weights='DEFAULT')  # Use updated weights parameter\n",
    "        self.resnet = nn.Sequential(*(list(self.resnet.children())[:-1]))  # Remove the last FC layer\n",
    "        self.fc = nn.Linear(2048, latent_space_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        latent = self.fc(x)\n",
    "        return latent\n",
    "\n",
    "# Instantiate the encoder and move to device\n",
    "encoder = ResNetEncoder().to(device)\n",
    "\n",
    "# Function to generate latent space for a dataset\n",
    "def generate_latent_space(model, data_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    latents = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, label in data_loader:\n",
    "            images = images.to(device)\n",
    "            latent = model(images)\n",
    "            latents.append(latent.cpu().numpy())\n",
    "            labels.extend(label.numpy())\n",
    "    \n",
    "    return np.concatenate(latents), np.array(labels)\n",
    "\n",
    "# Generate latent spaces for training and validation datasets\n",
    "print(\"Generating latent space for training data...\")\n",
    "train_latent, train_labels = generate_latent_space(encoder, train_loader)\n",
    "print(\"Generating latent space for validation data...\")\n",
    "val_latent, val_labels = generate_latent_space(encoder, val_loader)\n",
    "\n",
    "# Define individual models\n",
    "svm_model = SVC(kernel='linear', C=1.0)  # SVM\n",
    "knn_model = KNeighborsClassifier(n_neighbors=7)  # KNN\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)  # Random Forest\n",
    "xgb_model = XGBClassifier(objective='multi:softmax', num_class=len(train_dataset.classes),\n",
    "                          n_estimators=100, learning_rate=0.1, max_depth=6, use_label_encoder=False)  # XGBoost\n",
    "\n",
    "# Train each model on the latent space\n",
    "print(\"Training SVM...\")\n",
    "svm_model.fit(train_latent, train_labels)\n",
    "print(\"Training KNN...\")\n",
    "knn_model.fit(train_latent, train_labels)\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model.fit(train_latent, train_labels)\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model.fit(train_latent, train_labels)\n",
    "\n",
    "# Ensemble using VotingClassifier (hard voting)\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('svm', svm_model),\n",
    "    ('knn', knn_model),\n",
    "    ('rf', rf_model),\n",
    "    ('xgb', xgb_model)\n",
    "], voting='hard')\n",
    "\n",
    "# Train the ensemble classifier on the training latent space\n",
    "print(\"Training ensemble with hard voting...\")\n",
    "voting_clf.fit(train_latent, train_labels)\n",
    "\n",
    "# Predict on the validation latent space\n",
    "print(\"Predicting on validation data...\")\n",
    "val_predictions = voting_clf.predict(val_latent)\n",
    "\n",
    "# Evaluate the ensemble on validation data\n",
    "val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "print(f'Validation Accuracy (Hard Voting Ensemble): {val_accuracy:.4f}')\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(val_labels, val_predictions, target_names=train_dataset.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5b750d2-50b2-48d9-9cea-6f72d0319370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.0939, Acc: 0.6341, Rec: 0.6341, Prec: 0.6244, F1: 0.6241, Spec: 0.5730\n",
      "Epoch [2/50], Loss: 0.7837, Acc: 0.7337, Rec: 0.7337, Prec: 0.7309, F1: 0.7304, Spec: 0.6879\n",
      "Epoch [3/50], Loss: 0.6638, Acc: 0.7717, Rec: 0.7717, Prec: 0.7703, F1: 0.7697, Spec: 0.7329\n",
      "Epoch [4/50], Loss: 0.5897, Acc: 0.7966, Rec: 0.7966, Prec: 0.7954, F1: 0.7951, Spec: 0.7626\n",
      "Epoch [5/50], Loss: 0.5242, Acc: 0.8185, Rec: 0.8185, Prec: 0.8175, F1: 0.8174, Spec: 0.7888\n",
      "Epoch [6/50], Loss: 0.4737, Acc: 0.8362, Rec: 0.8362, Prec: 0.8355, F1: 0.8354, Spec: 0.8104\n",
      "Epoch [7/50], Loss: 0.4408, Acc: 0.8471, Rec: 0.8471, Prec: 0.8462, F1: 0.8463, Spec: 0.8227\n",
      "Epoch [8/50], Loss: 0.4066, Acc: 0.8585, Rec: 0.8585, Prec: 0.8575, F1: 0.8577, Spec: 0.8360\n",
      "Epoch [9/50], Loss: 0.3813, Acc: 0.8678, Rec: 0.8678, Prec: 0.8672, F1: 0.8673, Spec: 0.8479\n",
      "Epoch [10/50], Loss: 0.3532, Acc: 0.8776, Rec: 0.8776, Prec: 0.8770, F1: 0.8771, Spec: 0.8588\n",
      "Epoch [11/50], Loss: 0.3395, Acc: 0.8815, Rec: 0.8815, Prec: 0.8809, F1: 0.8811, Spec: 0.8636\n",
      "Epoch [12/50], Loss: 0.3235, Acc: 0.8894, Rec: 0.8894, Prec: 0.8888, F1: 0.8890, Spec: 0.8735\n",
      "Epoch [13/50], Loss: 0.3083, Acc: 0.8932, Rec: 0.8932, Prec: 0.8928, F1: 0.8929, Spec: 0.8777\n",
      "Epoch [14/50], Loss: 0.2939, Acc: 0.8981, Rec: 0.8981, Prec: 0.8977, F1: 0.8978, Spec: 0.8833\n",
      "Epoch [15/50], Loss: 0.2811, Acc: 0.9027, Rec: 0.9027, Prec: 0.9024, F1: 0.9025, Spec: 0.8885\n",
      "Epoch [16/50], Loss: 0.2708, Acc: 0.9066, Rec: 0.9066, Prec: 0.9062, F1: 0.9063, Spec: 0.8932\n",
      "Epoch [17/50], Loss: 0.2643, Acc: 0.9094, Rec: 0.9094, Prec: 0.9090, F1: 0.9091, Spec: 0.8961\n",
      "Epoch [18/50], Loss: 0.2544, Acc: 0.9109, Rec: 0.9109, Prec: 0.9105, F1: 0.9106, Spec: 0.8982\n",
      "Epoch [19/50], Loss: 0.2490, Acc: 0.9137, Rec: 0.9137, Prec: 0.9134, F1: 0.9135, Spec: 0.9013\n",
      "Epoch [20/50], Loss: 0.2409, Acc: 0.9164, Rec: 0.9164, Prec: 0.9161, F1: 0.9162, Spec: 0.9043\n",
      "Epoch [21/50], Loss: 0.2336, Acc: 0.9190, Rec: 0.9190, Prec: 0.9187, F1: 0.9188, Spec: 0.9069\n",
      "Epoch [22/50], Loss: 0.2299, Acc: 0.9203, Rec: 0.9203, Prec: 0.9200, F1: 0.9201, Spec: 0.9085\n",
      "Epoch [23/50], Loss: 0.2240, Acc: 0.9219, Rec: 0.9219, Prec: 0.9216, F1: 0.9217, Spec: 0.9100\n",
      "Epoch [24/50], Loss: 0.2180, Acc: 0.9238, Rec: 0.9238, Prec: 0.9235, F1: 0.9236, Spec: 0.9126\n",
      "Epoch [25/50], Loss: 0.2131, Acc: 0.9249, Rec: 0.9249, Prec: 0.9246, F1: 0.9247, Spec: 0.9142\n",
      "Epoch [26/50], Loss: 0.2118, Acc: 0.9258, Rec: 0.9258, Prec: 0.9255, F1: 0.9256, Spec: 0.9153\n",
      "Epoch [27/50], Loss: 0.2033, Acc: 0.9284, Rec: 0.9284, Prec: 0.9282, F1: 0.9282, Spec: 0.9182\n",
      "Epoch [28/50], Loss: 0.2005, Acc: 0.9291, Rec: 0.9291, Prec: 0.9289, F1: 0.9289, Spec: 0.9191\n",
      "Epoch [29/50], Loss: 0.2001, Acc: 0.9307, Rec: 0.9307, Prec: 0.9305, F1: 0.9305, Spec: 0.9204\n",
      "Epoch [30/50], Loss: 0.1918, Acc: 0.9328, Rec: 0.9328, Prec: 0.9327, F1: 0.9327, Spec: 0.9236\n",
      "Epoch [31/50], Loss: 0.1917, Acc: 0.9329, Rec: 0.9329, Prec: 0.9327, F1: 0.9328, Spec: 0.9230\n",
      "Epoch [32/50], Loss: 0.1872, Acc: 0.9337, Rec: 0.9337, Prec: 0.9335, F1: 0.9335, Spec: 0.9240\n",
      "Epoch [33/50], Loss: 0.1853, Acc: 0.9345, Rec: 0.9345, Prec: 0.9343, F1: 0.9344, Spec: 0.9247\n",
      "Epoch [34/50], Loss: 0.1814, Acc: 0.9363, Rec: 0.9363, Prec: 0.9361, F1: 0.9362, Spec: 0.9270\n",
      "Epoch [35/50], Loss: 0.1797, Acc: 0.9361, Rec: 0.9361, Prec: 0.9359, F1: 0.9360, Spec: 0.9267\n",
      "Epoch [36/50], Loss: 0.1760, Acc: 0.9380, Rec: 0.9380, Prec: 0.9379, F1: 0.9379, Spec: 0.9288\n",
      "Epoch [37/50], Loss: 0.1723, Acc: 0.9382, Rec: 0.9382, Prec: 0.9379, F1: 0.9380, Spec: 0.9289\n",
      "Epoch [38/50], Loss: 0.1739, Acc: 0.9385, Rec: 0.9385, Prec: 0.9383, F1: 0.9384, Spec: 0.9295\n",
      "Epoch [39/50], Loss: 0.1703, Acc: 0.9399, Rec: 0.9399, Prec: 0.9396, F1: 0.9397, Spec: 0.9311\n",
      "Epoch [40/50], Loss: 0.1656, Acc: 0.9417, Rec: 0.9417, Prec: 0.9415, F1: 0.9416, Spec: 0.9326\n",
      "Epoch [41/50], Loss: 0.1641, Acc: 0.9419, Rec: 0.9419, Prec: 0.9418, F1: 0.9418, Spec: 0.9333\n",
      "Epoch [42/50], Loss: 0.1596, Acc: 0.9434, Rec: 0.9434, Prec: 0.9433, F1: 0.9433, Spec: 0.9343\n",
      "Epoch [43/50], Loss: 0.1595, Acc: 0.9424, Rec: 0.9424, Prec: 0.9422, F1: 0.9422, Spec: 0.9337\n",
      "Epoch [44/50], Loss: 0.1576, Acc: 0.9444, Rec: 0.9444, Prec: 0.9443, F1: 0.9443, Spec: 0.9362\n",
      "Epoch [45/50], Loss: 0.1545, Acc: 0.9457, Rec: 0.9457, Prec: 0.9456, F1: 0.9456, Spec: 0.9374\n",
      "Epoch [46/50], Loss: 0.1516, Acc: 0.9462, Rec: 0.9462, Prec: 0.9461, F1: 0.9461, Spec: 0.9378\n",
      "Epoch [47/50], Loss: 0.1535, Acc: 0.9450, Rec: 0.9450, Prec: 0.9449, F1: 0.9449, Spec: 0.9366\n",
      "Epoch [48/50], Loss: 0.1510, Acc: 0.9462, Rec: 0.9462, Prec: 0.9461, F1: 0.9461, Spec: 0.9381\n",
      "Epoch [49/50], Loss: 0.1514, Acc: 0.9460, Rec: 0.9460, Prec: 0.9458, F1: 0.9459, Spec: 0.9377\n",
      "Epoch [50/50], Loss: 0.1447, Acc: 0.9486, Rec: 0.9486, Prec: 0.9484, F1: 0.9485, Spec: 0.9405\n",
      "Validation Loss: 0.3885, Acc: 0.9019, Rec: 0.9019, Prec: 0.9020, F1: 0.9018, Spec: 0.7597\n",
      "Test predictions saved to 'test_results_CBAM.csv'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Define CBAM Components\n",
    "class SAM(nn.Module):\n",
    "    def __init__(self, bias=False):\n",
    "        super(SAM, self).__init__()\n",
    "        self.conv = nn.Conv2d(2, 1, 7, padding=3, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_pool = torch.max(x, dim=1, keepdim=True)[0]\n",
    "        avg_pool = torch.mean(x, dim=1, keepdim=True)\n",
    "        concat = torch.cat([max_pool, avg_pool], dim=1)\n",
    "        output = torch.sigmoid(self.conv(concat)) * x\n",
    "        return output\n",
    "\n",
    "class CAM(nn.Module):\n",
    "    def __init__(self, channels, r):\n",
    "        super(CAM, self).__init__()\n",
    "        self.fc1 = nn.Linear(channels, channels // r)\n",
    "        self.fc2 = nn.Linear(channels // r, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_pool = torch.max(x, dim=2, keepdim=True)[0].max(dim=3, keepdim=True)[0]\n",
    "        avg_pool = torch.mean(x, dim=(2, 3), keepdim=True)\n",
    "        max_out = torch.relu(self.fc1(max_pool.view(x.size(0), -1)))\n",
    "        avg_out = torch.relu(self.fc1(avg_pool.view(x.size(0), -1)))\n",
    "        max_out = torch.sigmoid(self.fc2(max_out)).view(x.size(0), -1, 1, 1)\n",
    "        avg_out = torch.sigmoid(self.fc2(avg_out)).view(x.size(0), -1, 1, 1)\n",
    "        return (max_out + avg_out) * x\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, r=16):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.cam = CAM(channels, r)\n",
    "        self.sam = SAM()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cam(x)\n",
    "        x = self.sam(x)\n",
    "        return x\n",
    "\n",
    "# Define CBAM-Enhanced ResNet Model\n",
    "class CBAMResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CBAMResNet, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.fc = nn.Identity()  # Remove original FC layer\n",
    "        self.cbam = CBAM(512)  # 512 is the output channels from ResNet18\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)  # Output shape: (batch_size, 512)\n",
    "        x = x.unsqueeze(-1).unsqueeze(-1)  # Add height and width dimensions\n",
    "        x = self.cbam(x)  # Apply CBAM\n",
    "        x = x.view(x.size(0), -1)  # Flatten for FC layer\n",
    "        return self.fc(x)\n",
    "\n",
    "# Custom Dataset for Unlabelled Test Data\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.image_names = os.listdir(img_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_names[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_name\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "num_classes = 10\n",
    "train_data_dir = 'Dataset/new_train'\n",
    "val_data_dir = 'Dataset/validation'\n",
    "test_data_dir = 'Dataset/testing/images'\n",
    "\n",
    "# Data Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Datasets and Dataloaders\n",
    "train_dataset = datasets.ImageFolder(root=train_data_dir, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(root=val_data_dir, transform=transform)\n",
    "test_dataset = CustomImageDataset(img_dir=test_data_dir, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize Model, Loss Function, and Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CBAMResNet(num_classes=num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function to Compute Metrics\n",
    "def compute_metrics(preds, targets):\n",
    "    accuracy = accuracy_score(targets, preds)\n",
    "    recall = recall_score(targets, preds, average='weighted')\n",
    "    precision = precision_score(targets, preds, average='weighted')\n",
    "    f1 = f1_score(targets, preds, average='weighted')\n",
    "    cm = confusion_matrix(targets, preds)\n",
    "    specificity = cm.diagonal() / (cm.sum(axis=1) + 1e-6)\n",
    "    return accuracy, recall, precision, f1, specificity.mean()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_preds, train_labels = [], []\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        train_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    accuracy, recall, precision, f1, specificity = compute_metrics(train_preds, train_labels)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Acc: {accuracy:.4f}, Rec: {recall:.4f}, Prec: {precision:.4f}, F1: {f1:.4f}, Spec: {specificity:.4f}\")\n",
    "\n",
    "# Validation Loop\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_preds, val_labels = [], []\n",
    "    val_loss = 0.0\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        val_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "        val_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "        val_labels.extend(labels.cpu().numpy())\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy, recall, precision, f1, specificity = compute_metrics(val_preds, val_labels)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Acc: {accuracy:.4f}, Rec: {recall:.4f}, Prec: {precision:.4f}, F1: {f1:.4f}, Spec: {specificity:.4f}\")\n",
    "\n",
    "# Test Prediction and Report Generation\n",
    "test_results = []\n",
    "with torch.no_grad():\n",
    "    for images, img_name in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        probabilities = torch.softmax(outputs, dim=1).cpu().numpy().flatten()\n",
    "        predicted_class = probabilities.argmax()\n",
    "        result = {\n",
    "            'image_name': img_name[0],\n",
    "            **{f'class_{i}': prob for i, prob in enumerate(probabilities)},\n",
    "            'predicted_class': predicted_class\n",
    "        }\n",
    "        test_results.append(result)\n",
    "\n",
    "# Save Test Predictions to CSV\n",
    "df = pd.DataFrame(test_results)\n",
    "df.to_csv('test_results_CBAM.csv', index=False)\n",
    "print(\"Test predictions saved to 'test_results_CBAM.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac6d7521-f8b9-4324-80bc-b1f30e90ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cbam.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71047a54-0e3e-4aed-8709-7800744aac12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
